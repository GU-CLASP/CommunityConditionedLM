% -*- eval: (flyspell-mode 1); -*-

% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
%\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\input{preamble}

\title{Vector Representations of Linguistic Communities and Associated Models}
 % Representations -conditioned language models for linguistic variation

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Community-level linguistic variation is a core concept in sociolinguistics.
  % but more tools are needed to study variation from a computational perspective.
  In this paper, we use computational methods to investigate how much linguistic variation corresponds with social connections between communities.
  Technically, use conditional language models to investigate linguistic variation
  across 510 online communities.
  We probe the community representations learned by the conditional language model
  and find that they are highly correlated with a social network-based 
  representation that does not use any linguistic input.
\end{abstract}


% Introduction

% Community Conditioned Language Models:
%   Data sets
%   Architecture (with emphasis on the embeddings)
%   Training
%   Results (loss and loss per community)

% Latent bayesian community classifier
%    Formula (no training!)
%    Result 1: Confusion matrix
%    Transition: too much data; can we extract a more useful metric?
%    Indiscernibility: Formula, and table

% Correlation between linguistic embedding with social embeddings
%    Kumar2018 Data set
%    Explain the problem (idea: comparing cosine similarities --> too much data, again)
%    Sketch "procrustes" problem, explain what it yields. (Also what kind of similarity one gets with random input)
%    Results:
%      - Correlation between Kumar2018 and best LSTM
%      - Correlation between Kumar2018 and best Transformer 
%      - Correlation between Transformer and LSTM
%      - Concatenations ???



\section{Introduction}

Linguistic communication requires that speakers share
certain linguistic conventions, such as syntactic
structure, word meanings, and patterns of interaction.
Speakers assume that these conventions are \emph{common ground} among
their interlocutors, based on joint membership in a community
\cite{Stalnaker2002, Clark1996}.  Such \emph{speech communities} \citep{Gumperz1972} 
range in size from the very small, like members of a friend group, 
to the very large, like speakers of English. However, as \citet{Eckert1992}
point out, it is \emph{communities of practice}---defined by mutual social engagement
in a common activity---that are the primary locus of linguistic variation.

Variation is an important object of study in sociolinguistics,
and is naturally amenable to computational analysis \citep{Nguyen2016}.
Most previous computational work on linguistic variation 
has considered variation at the level of macro-social categories, such as gender
\citep{Burger2011,Ciot2013,Bamman2014}, age \citep{Nguyen2013}, and
geographic location \citep{Eisenstein2010,Bamman2014a}.%
\footnote{Recent exceptions are \citet{DelTredici2017} who use a modified
skip-gram model to show that lexical semantic variation occurs even across 
different communities organised around the same topic and 
\citet{Lucy2021}, who use contextualised word representations to
investigate community-level variation in word sense distributions.}
In the present work, we investigate linguistic variation
in online communities on the social media website Reddit. 
This contrasts with previous work: we investigate variation across 
speech communities, rather than macro-social categories.


%\jp{
  %Some examples (perhaps include if there is space):
%  
  %(CFB)
  %It was against SMU in the 47-48 Cotton bowl. They had cancelled a
  %game the year before against Miami for the same reason. There's
  %debate on if that's really the origin for the phrase but the story
  %itself is true.
%
  %(Videos)
  %Reminds me of Y2K computer bug. A lot of work went into
  %pre-emptively fixing it, and a lot of bugs were found and fixed,
  %some of which would have been catastrophic. Unfortunately, because
  %the work was successful, a lot of people now describe it as a waste
  %of time/money and a big deal made out of nothing.  }
%
% Indeed, it is folklore that certain communities tend to exhibit more predictable
% speech patterns than others.  Here we want to quantify this
% phenomenon.

For this purpose, we introduce (\cref{sec:cclm}) various
Community-Conditioned Language Models (CCLMs for short). These models
are conditioned on a vector representation (or embedding), which
varies for each community. Hence, they learn \emph{community
  embeddings}.  Even though we report which architectures make best use
of the community information, our primary purpose is not to improve models in
terms of perplexity, but rather we want to extract the best set of
community embeddings and test how the resulting embeddings
predict the social structure of subreddits. Technically, we test
the correlation with a social network-based representation of
communities (\cref{sec:embedding-analysis}).


% , and model the degree to which a
% given message $m$ is acceptable in a given community $c$, by estimating the probability
% \(P(m \mid c)\).

% To sum up, we test the following hypotheses, which we review in our
% conclusion (\cref{sec:discussion-conclusion}):
% \begin{hypotheses}
% \item \label{hyp:varying-complexity} Different communities have different levels of linguistic
%   complexity
% \item \label{hyp:layer-effect} The layer at which the community embedding is taken into account
%   in a CCLM influences its perplexity.
% \item \label{hyp:LMCC-works} CCLMs can be used to recognise communities based on
%   latent Bayesian classification.
% % \item \label{hyp:LMCC-works} The layer at which the community embedding is taken into account
% %   influences CCLM and its associated latent classifier.
%   % This is more of a justification for studying the latent classifier than a real "user-facing" hypothesis.
% \item \label{hyp:extra-linguistic-correlation} The CCLM representation of communities are correlated with
%   co-occurrence of users between communities.
% % \item Community stability is correlated with language complexity
% \item \label{hyp:rich-harder-to-identify} Linguistically poor communities are easier to identify than
%   linguistically rich communities.
% \end{hypotheses}

% We proceed as follows: 
% First, we define the CCLM models used in the paper, as well as baseline un-conditioned madels.
% We describe how the models were trained and report on overall perplexity results, 
% as well as comparing the perplexity of the CCLM models to the baseline at the community level
% .
% Next, we define latent Bayesian community classifiers based on the CCLM. We define a measure
% of \emph{indiscernability} for communities and report on correlations between indiscernability and perplexity .
% Then, we focus on the community embeddings learend by the CCLM,
% comparing them to a social network-based embedding .
% Finally, we give an overview of related work and discuss our findings in that context .


\section{Community-conditioned language models (CCLMs)}\label{sec:cclm}

We experiment with two kinds of model architecture: a simple
unidirectional LSTM \citep{Hochreiter1997} and a Transformer
\citep{Vaswani2017}. Although Transformer-based language models are considered
state-of-the-art, they achieve dominance partly thanks to the
availability of very large data sets \citep[e.g.,][]{Devlin2019,Brown2020},
which are not available to us.\footnote{Fine-tuning existing models
  is not compatible with our methodology, because we fundamentally
  change the structure of the network by concatenating
  community embeddings with hidden states at various levels.} Thus the LSTM is a worthy model to test for us.

In either case, the model is organised as a
standard $3$-layer neural sequence encoder, where the input for the
$t$th timestep of the $n+1$st layer is the $t$th hidden state of the 
$n$th layer.
As usual, the input to the first layer, is a sequence of tokens,
encoded with a trainable embedding layer over a pre-determined vocabulary.
At the other end, word tokens are predicted using a softmax projection 
layer. What we have described so far does not take community into account
and as such we call them \emph{unconditioned models}, but the same
encoder architecture also forms the core of our conditioned models.

In the CCLMs, we add a \emph{community embedding} parameter.
This parameter is concatenated (at each time step) with the hidden layer of the
sequence encoder, at some layer $l_c \leq n$, and passed through a
linear layer which projects the resulting vector back to the original
hidden layer size.  For $l_c = n$, the output of this linear layer is
passed directly to the softmax function, just as the final hidden
layer of the sequence encoder is in other models.  For $l_c=0$, the
community embedding is concatenated with the token embedding.  For
this reason, we set the hidden size of the sequence encoder and the
size of the token embedding to be equal for all models.


\subsection{Data sets}\label{sec:data}

We investigate linguistic variation across various communities 
from the social media website Reddit.\footnote{Comments were obtained
  from the archive at \url{https://pushshift.io/}.
  \cite{Baumgartner2020}. Code for reproducing our dataset, as well as
 our pre-trained community embeddings are available at URL.} %TODO ADD URL for camera ready
%
Reddit is divided into forums called \textit{subreddits}, 
which are typically organised around a topic of interest. 
Users create \textit{posts}, which consist of a link, image, 
or text, along with a \emph{comment} section. 
Comments are threaded: a comment can be made directly on a post,
or appear as a reply to another comment.
%
Hereafter we refer to such comments as ``messages'', matching our
convention in mathematical formulas: the letter $c$ stands for a
community, and $m$ stands for a message.

Our dataset includes messages from \num{510} subreddits, 
the set of all subreddits 
with at least \num{5000} messages per month for each month
of the year 2015.
Ignoring empty and deleted comments, we randomly sampled
\num{42000} messages from 2015 for each community.
We reserved \num{1000} messages from each community for development and testing,
leaving a total of \num{20.4}M messages for training.

Using \texttt{langid.py} \citep{Lui2012}, we observe that a majority of the overall
messages are classified as English (95\% of the test set) and 498 of \num{510} communities %TODO
have more than half of their messages classified as English.
Given the small amount of non-English data, 
we decided that the bias introduced by attempting to filter message
by language outweighted the potential benefits.\footnote{
  See \cref{sec:ethics} for futher discussion.}

Messages were preprocessed as follows:
we excluded the content of block quotes, code blocks, and tables and
removed markup (formatting) commands, 
extracting only rendered text.
Messages were tokenized using the default English model for the SpaCy tokenizer 
Version 2.2.3 \citep{Honnibal2017}.


\subsection{Training scheme}

The models use a vocabulary of \num{40000} tokens (including a special
out-of-vocabulary token), consisting of the most frequent tokens
across all communities.

We trained the models on a simple 
auto-regressive language modeling task with cross-entropy loss.  Because
the transformer operates on all tokens in the sequence at once, the
inputs to the model were masked and incrementally un-masked.
We used the AdamW
\citep{Loshchilov2019} optimisation algorithm, with an initial
learning rate of \num{0.001}, with no extra control on the decay
of learning rate.
%
The batch size was \num{256} and the maximum sequence length set to
\num{64} tokens, truncating longer messages (16.8\% of messages were
longer than \num{64} tokens).  During training, a dropout rate of
$0.1$ was applied between encoder layers and after each linear layer.

All experiments use models with \num{3} encoder layers,
each with hidden (and token embedding) size of \num{256}. 
The transformer models had \num{8} attention heads per layer.\footnote{
  This number of attention heads was chosen to give the LSTM and transformer
  models a comparable number of parameters 
  (\num{22171203} and \num{21779523}, respectively).}
The conditional models were given a community embedding with \num{16} dimensions. 
We experimented with every possible value for $l_c$ in a three-layer model ($l_c\in\{0,1,2,3\}$).

We trained the models until the validation loss stopped decreasing for
two epochs in a row, and used the weights from the epoch with the
smallest validation loss for testing. 
Each training epoch took approximately 1.5 hours of GPU time.

%In general, transformer models
%were trained for about half as long as the LSTM models (\cref{tab:model-results}, 
%test epoch).

\section{CCLM Performance}

In this section, we report the performance of the conditioned and un-conditioned
models on the held out test set.
First, we define two performance metrics: perplexity and information gain.
In the following, we use $M$ to refer to messages in the
combined test set, and $M_j$ for the partition of the test set originating from 
community $c_j$.

\subsection{Perplexity}\label{sec:ppl}

For a given model, let $H(m)$ be the model's cross-entropy loss,
averaged over tokens in $m$.
We define the perplexity on a set of messages, $M$,
to be the exponential of the model's average cross-entropy loss:
\[\Ppl_M = e^{\avg_{m\in M} H(m)}\]

\paragraph{CCLM Information Gain}\label{sec:ig}

We also consider the average information gain per token of the CCLM over its baseline
un-conditioned counterpart, with the same sequence encoder architecture.
For a given message, information gain is defined as the difference
between the cross-entropy of the unconditioned model and the conditioned model:
\[H_{\mathrm{LM}}(m) - H_{\mathrm{CCLM}}(m)\]
For a set of messages, $M$, we consider the average information gain
in exponential space (as a ratio of perplexities):
\[\IG_M = \frac{e^{\avg_{m \in M}(H_{\mathrm{LM}}(m))}}{e^{\avg_{m \in M}(H_{\mathrm{CCLM}}(m))}}\]
\[\IG_M = {e^{\avg_{m \in M}(H_{\mathrm{LM}}(m) - H_{\mathrm{CCLM}}(m))}}\]
%
Unsurprisingly, the conditioned models mostly have lower perplexity 
than their respective unconditioned baseline models, 
meaning information gain above one (\cref{tab:model-results}).
While the absolute performance ($\Ppl_{M_j}$) of the LSTM models is better,
the best transformer models have somewhat higher information gain
than their LSTM counterparts.

The effect of $l_c$, the depth of the community embedding,
is also different across architectures.
For the LSTM encoder, 
the best model concatenates the community embedding after the first encoder layer ($l_c=1$),
but all of the conditioned models perform similarly well.
For the transformer, the best model incorporates the community information
first, concatenating it directly to the word vectors ($l_c=0$).
It performs similarly to the model that only integrates the community information
after all all the transformer layers ($l_c=3$),
but the two middle-layer models actually perform worse than the unconditioned model
(with $\IG_M < 1$).

We additionally observe that the best LSTM model achieves better
performance than the best Transformer model. This partly contradicts
the fact that Transformer-based models are the ones used in
state-of-the art models for mainstream tasks. We propose that the
cause is that we are training all models from scratch on a relatively
small dataset. In this situation, the relatively smaller number of
parameters of the LSTM means that it can generalise better than the
Transformer.

\begin{table}
  \small
  \centering
  \begin{tabular}{llrrr}
  \toprule
                                 &       & \makecell{test \\ epoch} & $\Ppl_M$ & $\IG_M$     \\
                                 & $l_c$ &         &                 &                   \\
  \midrule
  \multirow{5}{*}{LSTM}         & - &          12 &  68.74          &       -           \\
                                & 0 &          13 &  66.16          &    1.039          \\
                                & 1 &           7 &  \textbf{66.01} &    \textbf{1.041} \\
                                & 2 &           4 &  66.19          &    1.039          \\
                                & 3 &           4 &  66.35          &    1.036          \\
  \midrule
  \multirow{5}{*}{Transformer}  & - &           4 &  79.13          &        -          \\
                                & 0 &           4 &  \textbf{75.66} &    \textbf{1.046} \\
                                & 1 &           4 &  82.12          &    0.964          \\
                                & 2 &           7 &  83.53          &    0.947          \\
                                & 3 &           3 &  75.90          &    1.043          \\
  \bottomrule
  \end{tabular}
  \caption{
    Performance of baseline (first row for each encoder architecture)
    and CCLM models.  The scope of perplexity and information gain
    ($M$) is the entire test set, i.e. $5000×510$ messages; \num{5000}
    for each community.}
  \label{tab:model-results}
\end{table}

We also consider performance stratified by community; that is,
$\Ppl_{M_j}$ and $\IG_{M_j}$, 
where $M_j$ is the set of messages originating from community $c_j$ 
(\cref{fig:comm-stratified-box}).
We observe a lot of variation in baseline perplexity
across communities, with $\Ppl_{M_j}$ ranging from \num{3.67} to
\num{93.58} for the best conditional LSTM model 
(\cref{fig:comm-stratified-box}; also see supplementary materials for perplexity by community). 
The conditioned models also perform differently across different communities---%
even among the best models, some communities have $\IG_{M_j} < 1$,
meaning that the CCLM performs worse than the unconditioned baseline for 
messages from that community.
For other communities $\IG_{M_j}$ is much higher, meaning that the CCLM performs better  
(\cref{fig:comm-stratified-box}).\footnote{
  Some of the communities with consistently high $IG_{M_j}$ 
  across all models are primarily non-English, 
  but surprisingly, not the three most extreme
  outliers. There are \subreddit{counting}, \subreddit{friendsafari}, and \subreddit{Fireteams}, 
  the later two of which are places where people coordinate to play video games
  together. The messages in these communites adhere to highly regular formats, 
  which are presumably conventional to the community.}
%
%The variation in information gain across communities is not 
%related to perplexity, however (\cref{fig:ppl-info-gain}).



\newcommand{\modelboxplot}[3]{
  \addplot+[
    boxplot={draw position=#3, box extend=0.3}, 
    draw=#2, mark=*, mark options={fill=#2, scale=0.5}, solid, fill=#2!10,
    area legend] 
    table [y=#1] {floats/comm.csv};
}
\begin{figure}[t]
\begin{tikzpicture}
  \begin{axis}[name=ig,
    height=7cm,width=8cm,
    boxplot/draw direction=x, axis x line*=bottom, axis y line*=left,y axis line style={opacity=0},
    xmax=1.6,
    ylabel=$l_c$, xlabel=$\IG_{M_j}$,
    legend style={font=\tiny,at={(1,1)},anchor=north east}, legend columns=2
    ]
    \addplot[thin, red, samples=2] coordinates {(1,-0.5)(1,3.5)};
    \foreach \lc in {0, ..., 3}{
      \modelboxplot{lstm-3-\lc-ig}{violet}{\lc+0.2}
      \modelboxplot{transformer-3-\lc-ig}{teal}{\lc-0.2}
    \legend{,LSTM, Transformer}
    }
  \end{axis}
  \begin{axis}[name=ppl,
    anchor=north west,at={($(ig.south west)-(0,1.1cm)$)},
    width=8cm, height=2.5cm, 
    boxplot/draw direction=x, axis x line*=bottom, hide y axis,
    xlabel=$\Ppl_{M_j}$,
    ]
    \modelboxplot{lstm-3-ppl}{violet}{0.2}
    \modelboxplot{transformer-3-ppl}{teal}{-.2}
  \end{axis}
\end{tikzpicture}
\caption{
  Average model performance by community.
The boxes indicate upper and lower quartiles, while the whiskers are placed at the upper and lower maximum, with communities more than $1.5\times IQR$ (inter-quartile range) above the upper quartile considered outliers (represented as dots). The three most extreme outliers are excluded from this view.}% \cref{fig:ppl-info-gain}).
   %See also table 1 in the supplementary materials % not very useful for this actually since we sort by indisc and it's only one model anyway.
   %for a tabular version of these results with community names.
\label{fig:comm-stratified-box}
\end{figure}

We observe that across all the models we
tested, communities where conditioning has the least effect tend to be
organised around more general interest topics, such as 
\subreddit{relationships} and \subreddit{advice}, where the subject
matter is relevant to a broad range of people.  Conditioning the model
on community appears to have the most benefit for narrower special-interest
subreddits, such as those organised around a certain videogame, sports
team, or subculture.  These empirical observations corroborate the idea that communities of practice
are the primary locus of linguistic variation.
%makes sense intuitively, since communities
%with more niche topics would tend to have more specialised vocabulary
%and other linguistic patterns.

\section{Comparison of CCLM community embeddings with a social network embedding}\label{sec:embedding-analysis}



In this section we investigate the degree to which CCLM community
embeddings correlate with the community-member relationship on Reddit.

To this end, we compare the CCLM-learned community
embeddings\footnote{
  In this section, we only consider the embeddings from 
  the \emph{best} (highest information gain) CCLM from each architecture family; 
  that is, the LSTM with $l_c=1$ and the Transformer with $l_c=0$, however we
  observed similar results for other values of $l_c$.}
with the community embedding created by \citet{Kumar2018},\footnote{
  Available at \url{https://snap.stanford.edu/data/web-RedditEmbeddings.html}}
which were generated using using a negative-sampling optimization
algorithm, with the message author-community co-occurrence matrix as ground
truth, using data from January 2014 to April 2017.
We refer the reader
to \citet{Kumar2018}
for details, but the important point is that no linguistic
information is used to create these embeddings: they only reflect the social
relationship between communities, via 
the user-community membership relation. In contrast, CCLM community embeddings depend in no
way on which user is the author of any given message: we only use the
contents of messages, not authorship data.
%

\subsection{Comparing embeddings: cosine similarities}
\label{sec:storytelling}

\newcommand{\pairwisecossimbarplot}[1]{
    \nextgroupplot[ybar=0pt,
      bar width=0.25cm,
      xticklabels from table={floats/#1}{comms-label},
      enlargelimits=0.15,
      yticklabels={,,}
    ]
    \addplot[draw=violet,fill=violet!10] table [y=lstm-sim] {floats/#1};
    \addplot[draw=teal,fill=teal!10] table [y=transformer-sim] {floats/#1};
    \addplot[draw=red,fill=red!10] table [y=social-sim] {floats/#1};
}

\begin{figure*}[ht]
\begin{tikzpicture}
  \begin{groupplot}[group style={
      group size=3 by 1,
      horizontal sep=0.5cm,
    },
    height=5cm,width=0.39\textwidth,
    axis x line*=bottom, axis y line*=left,x axis line style={opacity=0},
    ymin=-0.35, ymax=1.00,
    xtick=data,
    xticklabel style={align=right, anchor=east, rotate=60},
    minor tick length=1ex,
    legend style={font=\tiny,at={(1,1)},anchor=north east}, legend columns=3
    ]
    \nextgroupplot[ybar=0pt,
      bar width=0.25cm,
      xticklabels from table={floats/high_ling_high_soc.csv}{comms-label},
      enlargelimits=0.15,
    ]
    \addplot[draw=violet,fill=violet!10] table [y=lstm-sim] {floats/high_ling_high_soc.csv};
    \addplot[draw=teal,fill=teal!10] table [y=transformer-sim] {floats/high_ling_high_soc.csv};
    \addplot[draw=red,fill=red!10] table [y=social-sim] {floats/high_ling_high_soc.csv};
    \pairwisecossimbarplot{high_ling_low_soc.csv}{}{}
    \pairwisecossimbarplot{low_ling_high_soc.csv}{}{}
    \legend{LSTM, Transformer, Social}
  \end{groupplot}
\end{tikzpicture}
\caption{
  Cosine similarity between pairs of communities, computed for vectors from the
  best CCLM embeddings (LSTM: $l_c=1$, Transformer: $l_c=0$)
  and the social embedding from \citet{Kumar2018}.
  Communities with high linguistic and social similarity \textbf{(left)},
  high linguistic but low social similarity \textbf{(center)}, and
  low linguistic but high social similarity \textbf{(right)}.
  See text for details on the selection criteria.}
  \label{fig:embedding-differences-examples}
\end{figure*}

When comparing social embeddings and linguistic embeddings, a difficulty is that they
range over completely unrelated spaces. Thus one cannot use the usual
cosine similarity metric \emph{between} these spaces. One can,
however, use cosine similarity between \emph{pairs} of communities,
and verify that the similarities are correlated between linguistic and social
embeddings.  This gives a way of characterizing the differences
between the two kinds of community representation. To get a more
concrete sense of what this method yields, we first survey some of the
most salient community pairs. We stress that this survey is not meant
as a rigorous statistical analysis, as we shall see. Rather it is
meant to give a flavor of discrepancies and similarities existing
between linguistic and social relations.

We consider communities from three different selection criteria: 
Those with high linguistic \emph{and} social similarity 
(where the sum of the two is highest),
those with high linguistic and low social similarity
(where social similarity is below the median and linguistic similarity is highest), and
those with low linguistic and high social similarity
(where linguistic similarity is below these median and linguistic similarity is highest).\footnote{We use the LSTM ($l_c=1$) community vectors for these purposes, 
  but results attain with the best transformer model.}\footnote{Median similarity
  among pairs of communities was \num{0.177} for the social embedding
  and \num{0.010} and \num{0.012} for the LSTM and transformer
  linguistic embeddings, respectively.}
We do not consider pairs of communities that are different in both ways,
since these don't offer much in the way of understanding the respective embeddings.

Unsuprisingly, the first category (\cref{fig:embedding-differences-examples}, left) yields communities that are qualitatively very similar. 
The \subreddit{SSBPM} and \subreddit{darksouls} communities are focused
around discussion of a particular videogame, and are paired
with communities that discuss a variation of the same game.
The \subreddit{amiugly} and \subreddit{Rateme} communities are both 
forums where the posts are selfies and the comments are mostly 
comments on the person's appearance.
The two communities paired with \subreddit{reddevils} are 
likewise comprised of fans of a particular English football club.
That the three football club communities are so closely related points
to an important point about the social embedding: because of the way
it was trained, it does not have to be the case that communities with
a similar representation have a high overlap in membership,
but rather that members from the first community look similar
to members from the second community in terms of their \emph{overall}
community membership profile.

Communities with similar linguistic embeddings but dissimilar social embeddings
(\cref{fig:embedding-differences-examples}, left) tend to share a similar
topic, mode of interaction, or language variety, but 
in all cases we looked at, there is some reason to expect that
they might nevertheless attract different members.
For example,
\subreddit{hiphopheads} and \subreddit{Monstercat} are both topically
related to music, but the music genres are different, and the later
has a more geographically local focus (Monstercat is an independent
electronic music label based in Vancouver). The interactions in both 
\subreddit{MLPLounge} and \subreddit{CasualConversation} 
could be described as casual conversation, the former is intended specifically for
members of a niche internet sub-culture. % adult fans of the children's television show My Little Pony. There are other subreddits (mentioned in r/MLPLounge's "about") that are for on-topic discussion of MLP
The \subreddit{exmormon} and \subreddit{Catholicism} communities discuss
the Mormon and Catholic churches, although their members have different 
relationships towards those organizations---the former is intended for
former members of the church, whereas the later is geared towards practicing
Catholics.
Finally, both \subreddit{rocketbeans} and \subreddit{de} are primarily
German-language subreddits, but the former is comprised of fans of a
computer gaming YouTube channel, while the later is more
general-interest.

Differences at the other end of the spectrum
(\cref{fig:embedding-differences-examples}, right) are somewhat harder
to interpret.  It is mostly easy to see why these communities would
have different linguistic embeddings---%
in all cases the topics are quite different.  The reason they have
similar social embeddings is less obvious, but we can discern some
trends in how the communities are premised.  The
\subreddit{progresspics} and \subreddit{TalesFromRetail} are premised,
in part, on seeking support from other people with similar
experiences; \subreddit{legaladvice}, \subreddit{Cooking}, and
\subreddit{loseit} all involve sharing knowledge on a particular topic;
\subreddit{running} and \subreddit{Coffee} are hobby-focused; and
\subreddit{self} (often) and \subreddit{askscience} (by premise) are
places people ask and answer questions.  It may be that there are
different patterns in the \emph{social function} that people attribute
to this particular social media website---%
people who use Reddit in one way are more likely to belong to
communities that are premised on the same kind of social function,
even if the topics (and indeed language) of those communities are
quite different.  Testing this hypothesis would require a more focused
study design and ideally consider communities from multiple social
networks (online or otherwise).

In sum, empirical observation simultaneously reveals examples of high
and low correlation between social and linguistic embeddings.  To
quantify correlation and extract the general trends, we must resort to
statistical tools, as we do below.

A straightforward (but ultimately flawed) way to measure how similar
the two spaces are would be to generalise the above method, by
consider each pair of communities \((i,j)\), and compute the
correlation between the cosine similarities of both embeddings.

That is, we can compute the Pearson correlation factor of the data set:
\[C = \{(x = L_i · L_j, y = S_i · S_j) \text {~for~} i,j ∈
  [1,510]\}\] % BN: changed \C to C. Definition for \C may be missing
where \(L_i\) and \(S_i\) are the linguistic and social embeddings for
community \(i\).  (Thus \(L\) is the matrix of (normed) linguistic
embeddings and \(S\) the matrix of (normed) social embeddings.)

The analysis shows positive correlation for both the LSTM ($r=0.438$)
and transformer ($r=0.452$) linguistic embeddings.\footnote{ By
  comparison, the correlation between the two linguistic embeddings is
  \num{0.759}.}  The correlations are significant with $p<0.001$ in
all cases.  However, we note that the number of pairs grows with the
square of the number of communities (with 510 communities, we have
129795) pairs), meaning that standard statistical tests on Pearson
correlation will assure us of statistical significance in all but the
weakest of correlations. A further flaw is that the data points in
\(C\) are \emph{not} distributed independently---far from it in fact,
since each data point is generated from \num{2} of \num{510}
independent variables. We consider this last flaw fatal, and take a
different approach for computing the correlation between community
embeddings in the next section.


\subsection{Comparing embeddings: Procrustes method}
\label{sec:statistical-evidence}



The anecdotal evidence suggests that for some communities, proximity
in linguistic patterns do match social proximity and for others it
does not. Unfortunately, it seems hard, if not impossible, to draw a
general conclusion from this mode of analysis. In this section, we propose a
systematic approach with which we can quantify the correlation
between social proximity and linguistic proximity, and measure its
statistical significance.

% Explain the problem (idea: comparing cosine similarities --> too much data, again)

Instead of comparing embedding pairs, as in \cref{sec:storytelling}, 
we will compare embeddings community by community.
A naive approach would be to 
calculate the distance between two embeddings index-wise, which is
equal to the Frobenius distance between \(L\) and \(S\):

\[||L-S||_F = \sum_i (L_i - S_i)\]

The problem with this approach is that even if several dimensions of
\(L\) and \(S\) are correlated, they will not coincide in the
\emph{representation}. That is, applying a simple rotation (orthogonal
transformation) on either matrix widely changes the \(||L-S||_F\)
correlation metric. Another way to consider the same problem is that,
applying such a rotation does \emph{not} change the cosine similarities
considered in the previous section.

To make the metric independent of the representation (up to orthogonal
transformations), we compute the \emph{minimum} distance between
\(L_i\) and \(S_i\), for any orthogonal matrix \(\Omega\) applied to
\(L\):

\[d(L,S) = \mathsf{argmin}_\Omega ||ΩL-S||_F\]

Here, the orthogonal matrix \(Ω\) gives a map from linguistic embeddings to social embeddings.
The problem of computing \(d(L,S)\) is known as the orthogonal Procrustes problem~\citep{Gower2004}.\footnote{
This approach has also been used to compare word embeddings across representations \citep[e.g.,][]{Hamilton2016a}.}
The solution is
\[d(L,S) = n - \mathsf {Tr} (Σ)\] where the matrix \(Σ\) is obtained by the singular value
decomposition (SVD) \(U^TΣV = LS^T\). The vectors of \(U\) and \(V\) give
the directions of correlation respectively of \(L\) and \(S\). That is, each
singular value \(\sigma_i\) (the elements of the diagonal matrix \(Σ\)), give a measure of how much
correlation there is between the directions \(U_i\) and \(V_i\).

As is common when doing SVD, we arrange \(U\), \(V\) and \(Σ\) such
that \(σ_i > σ_j\) iff \(i < j\). Doing so, the largest singular value
\(σ_0\) corresponds to the principal directions of correlation
(\(U_0\), \(V_0\)), \(σ_1\) to the second principal direction, etc.

The \(d(L,S)\) metric ranges from \(0\) (corresponding to perfect
correlation, obtained for example if \(L=S\)) to \(n\) (corresponding
to perfect orthogonality), where $n=510$ is the number of communities
considered.

Now, to test if \(d(L,S)\) corresponds to a significant correlation,
it suffices to check if its value is significantly larger than the same
value for random linguistic embeddings \(L\). Even though this is the
distribution of \(d(L,S)\) for random embeddings is difficult to
compute analytically, one can instead evaluate it using a Monte Carlo
method.

Doing so, we observed that \(d(L,S)\) exhibits a mean of
\(μ_d=431.39\) and a (Bessel's-corrected) standard deviation
\(s_d=2.90\) in their distance from the social embedding,
\(S\). % luckily it seems that sample stdev is often written with an $s$ to differentiate from population stdev


Thus if the real \(d(L,S)\) is below the mean by several standard
deviations, we can safely assume that there is statistically
significant correlation between \(L\) and \(S\). A 4-sigma difference
has less than one percent chance of occurring randomly. In our case,
we observe a difference of between 61 and 68 standard deviations
(\cref{tab:embedding-correlations}). This definitely indicates a
significant correlation.  Furthermore, by coming back to the
definition of \(d(L,S)\), we know that, on average, the cosine
similarity between \(ΩL\) and \(S\) is \(0.45 = \) (\(510 - 232.18 / 510\)).  It
further means that if we obtain a linguistic embedding \(L_k\) for a
new community $k$, we can estimate its social embedding by \(ΩL_k\),
and the cosine similarity with its true social embedding \(S_k\) is
expected to be \(0.39\) = \((431.39 - 232.18) / 510\) --- accounting for
over-fitting effects by taking the average distance rather than the
maximum. 
In sum, it is clear that the CCLM embeddings predict some aspect the social-network embeddings--- but far from all of it.

To finish, we also give a sense of \emph{how} the correlation is manifested overall, by
analysis of the two principal components of correlation in the
linguistic embeddings, \(U_0\) and \(U_1\). To do so we plot the
projection of each embedding along their first two principle components
which, together with the corresponding singular values, gives an idea
of how much and in what way they differ (see~\cref{fig:pca-aligned}).

\input{floats/pca_aligned}
\begin{figure*}[t]
    \centering
    \PCAAligned 
  \caption{First two components of the aligned social (left) and linguistic (right) embeddings,
    where the lingusitic embedding is taken from the LTSM with $l_c=1$.
    Correlation between these directions is given by $\sigma_0 = 53.4$ and $\sigma_1 = 35.6$.
    Colors are assigned by k-means clustering of the social embedding. 
    This figure is reproduced in the supplementary materials with a legend that helps 
    to characterise the clusters.
  }
  \label{fig:pca-aligned}
\end{figure*}



\begin{table}
\centering
\begin{tabular}{llcc}
\toprule
                        &   & LSTM     & Transformer \\
\midrule
  \multirow{4}{*}{$l_c$}  & 0 &         254.06  (61.21) &         239.41  (66.79)  \\
                          & 1 &         245.14  (64.29) & \textbf{232.18} (68.54)   \\
                          & 2 &         249.17  (62.90) &         233.47  (68.32)  \\
                          & 3 & \textbf{241.13} (65.67) &         237.74   (66.84) \\
\bottomrule
\end{tabular}
\caption{Distance between CCLM embeddings and the social network-based embedding
of \citet{Kumar2018}, as measured by $d(L,S)$. In parentheses is the number of standard deviations
from the mean distance of our random embedding samples.} \label{tab:embedding-correlations}
\end{table}

% (Another idea may be to correlate the cosine similarity between
% \((S_i,S_j)\) and \((L_i,L_j)\). But this would yield one number for
% each pair of communities \((510 × 509 / 2)\): too much data to deal with,
% and it is hard to get a sense of the meaning, let alone statistical significance of the
% resulting correlation coefficient.)\jp{Not sure if this discussion has its place here.}


% \subsection{Results}

%    Results:
%      - Correlation between Kumar2018 and best LSTM
%      - Correlation between Kumar2018 and best Transformer 
%      - Correlation between Transformer and LSTM
%      - Concatenations ???


\section{Related work}\label{sec:related-work}

We have presented results using conditional neural language models
to model variation between speech communities.
The architecture of these models concatenates a vector representation
of the conditioned variable to the input of the sequence model.
This approach has been applied in various conditioned text generation domains such as 
image captioning \citep{Vinyals2015}, machine translation \citep{Kalchbrenner2013},
but it has not, to our knowledge, been used extensively to study linguistic variation.

There are, however, related applications of conditional neural language models.
\citet{Lau2017a} presents a neural language model that jointly learns to predict
words on the sentence-level and represent topics on the document level.
The topic representation is then fed back into the language model, 
improving its performance on next word prediction.
This is similar to how our model experiences improved performance
by learning community representations. 
Unlike our model, topics are inferred in an unsupervised way, 
raising the question of whether communities could be identified from 
unlabeled data as well.

Along these lines, \citet{OConnor2010} uses a Bayesian generative
model to infer communities from variation in text data.  In contrast
to our work, this model treats words as independent events, ignoring
the structure (and variation) in the construction of sequences.  It
does further suggest, however, that community-level variation can be
modeled in an unsupervised way.

There are several recent studies that aim to measure \emph{linguistic distinctiveness}
at the level of speech community \citep{OConnor2010,Zhang2018,Lucy2021}, 
which is an possible interpretation of the community-stratified information gain 
of the CCLM over its unconditioned counterpart \cref{sec:ig}. 
Whereas the metrics in previous work are based on lexical frequency
(and in the case of \citet{Lucy2021}, word sense distributions),
CCLM information gain is capable of capturing 
distinctiveness at multiple levels of linguistic analysis. 
However, further work is needed to
investigate exactly what kinds of variation are captured.

While the focus of the paper is theoretical,
computational models of variation can also support 
robust, equitable language technology.  Previous work has shown that
speaker demographics can improve performance on standard NLP tasks
\citep{Hovy2015,Yang2017}. %\jp{Move this to related work so we can get to the point quicker.} 

%- DelTredici 2017
%- Einstein 2010
%- Baman et al 2014/2014a
%- Martin 2017 (community2vec), Kumar 2018
%- Huang 2014
%- Lucy 2021

\section{Discussion and Conclusion}\label{sec:discussion-conclusion}


To sum up our findings, we have defined community-conditioned language
models (CCLMs). These models are generally able to attune to
community-specific language, as witnessed by the information gain that
they exhibit over baseline unconditioned models.

We find that the layer depth of the community embedding 
($l_c$) has a weak effect on the information gain and the
perplexity of the CCLMs.

For LSTM models, the perplexity per word, averaged over
messages from all communities, was between \num{66.01} and \num{66.35}
(with \num{68.74} for the unconditioned model).
For transformer models, it varies a bit more, between \num{75.66} and \num{83.53},
but this seems to be mainly due to the poor performance of the models where
the community embedding is inserted between transformer layers
($l_c=2$ and $3$ both test above the unconditioned transformer's 
average perplexity of \num{79.13}).
This could be due to the community embedding and linear layers
interrupting the delicate layer normalisation between transformer encoder layers.

The pattern of information gain by community is similar across
architectures;  communities that benefit most from the conditioned
model behave that way for both the LSTM and transformer.  However,
there are some differences.  For example, many of the communities with the biggest 
difference in information gain between the $l_c=0$ and $l_c=3$ LSTMs 
are organised around trading collectables or organising virtual meetups
(e.g.,
%\subreddit{friendsafari},
%\subreddit{Fireteams},
%\subreddit{RandomActsOfGaming},
%\subreddit{randomactsofcsgo},
\subreddit{Pokemongiveaway},
\subreddit{ACTrade}, and
\subreddit{SVExchange}).  
These communities tended to have highly conventionalized ways negotiating trades
and coordinating meetups.
%Why was the model where the community information was incorporated before
%the LSTM able to take better advantage of it for these communities in particular? 
It would be interesting to investigate these differences further in future work, since it
could reveal differences in the kind of linguistic variation the
different model architectures capture.


% we observed that various communities exhibit widely varying levels of
% linguistic complexity \ref{hyp:varying-complexity}. For the best model architecture,
% perplexity per-word ranges from less than \num{4} to more than \num{93} 
% (but with only 25\% of communities under 63).

% With indiscernibility, values are distributed evenly over the whole possible
% range (1 to 510). We find that CCLMs can be used to recognise certain
% communities based on latent Bayesian classification
% \ref{hyp:LMCC-works}, but not all.  In general, we observe that the least
% discernible communities all have a very conversational, general-interest
% premise. We cannot rule out that these more generic
% communities exhibit distinctive linguistic patterns, but none of our
% CCLMs are able to discern them. In contrast, the distinctive patterns
% of narrow-topic communities are just as recognizable by all our LSTM
% and Transformer models.

% We find a mild correlation (\cref{tab:ind-corrs}) between the
% perplexity of the CCLM and the indiscernibility of the corresponding
% community \ref{hyp:rich-harder-to-identify}, with factors ranging from
% 0.06 to 0.23. However, The trend is more clear when considering the
% correlation with information gain and indiscernibility (correlation
% from 0.39 to 0.46). So this means that, when trying to identify a
% community, it is not its absolute perplexity that matters, but rather
% the difference in perplexity with the baseline.

Our main result is that community representations learned by
CCLMs are positively correlation with user co-occurrence patterns.
Even though such \emph{homophilic} correlation is a core hypothesis of 
sociolinguistics (see \citet{Kovacs2020}, for example), 
we believe that this study is the first to test it at 
the level of communities of practice using computational methods.
Furthermore, it appears that our method (correlating
linguistic embeddings and social embeddings) is novel. Indeed, even
though the Procrustes method has been used to correlate two sets of
linguistic embeddings \emph{for the same model}, we find no evidence
of the method being applied to embeddings for widely different models,
as we have done.

% A natural
% question that may arise is how much this correlation can be explained by
% user-specific linguistic patterns. We plan to control for this variable
% in future work, for example by building author-conditioned language
% models and testing if community vector embeddings can be determined by
% the set of the vector embeddings of its members.

\section{Ethical considerations}\label{sec:ethics}

\paragraph{Data privacy}
Our work uses publicly available data from Reddit,
collected from the API made available by \citet{Baumgartner2020}.
Additional considerations apply, however 
(see \citet{Gliniecka2021} for discussion). Reddit users are not,
in general, aware of the possibility that their data will be used 
for research purposes, and deleted posts can persist in archive formats.
We do not release any data, since the it is already publicly available 
and duplicating the dataset increases the likelihood that deleted 
posts will persist.

The paper does not include any text that could be linked back to 
personally identifiable information.
We do release our trained community embeddings, but they have 
low dimensionality and pose a low risk for exposing personally
identifiable information.

\paragraph{Language identification}
As mentioned in section \cref{sec:data}, we decided not to filter our data for non-English 
comments.
Although our focus in this paper is intra-language variation, 
language identification has the potential to introduce bias by reinforcing hegemonic
language classes and the boundaries between them.
In our case, filtering out messages classified as non-English would 
introduce bias by disproportionately removing messages in
non-standard and code-switched language varieties,
which are of interest in the current work.

Nevertheless, the representations learned by our model are (necessarily)
relative to the other communities in the dataset. 
Thus the learned representations for non-English communities tend to be more similar to each other 
than to other communities that use mostly English, even if their predominant language is not the same.
This would probably not be the case if the distribution of messages 
was more varied across hegemonic language classes; our work cannot be used to conclude,
for example, that there is more variation within English than between Dutch and German.

\paragraph{Subjective analysis}
In the qualitative discussion offered in \cref{sec:storytelling},
our comparative characterization of the topic, mode of interaction, 
and language varieties
used in the pairs of communities were formed by reading comments from the data our 
language models were trained on. This included Googling words and phrases that 
were unfamiliar. 
Where we make claims about the how the community is ``premised'' or what kinds of 
members it is ``geared towards'' or ``intended for'', these are based on the text
of the sidebar on the community's Reddit page.
While we believe this methodology, aggregated over many pairs of communities, 
is appropriate for making a qualitative comparison
of the community features encoded by different representations, 
to make conclusions about \emph{particular} communities based on such a cursory and
subjective analysis would be dubious and potentially harmful.

\bibliography{paper}
\bibliographystyle{acl_natbib}

\include{appendix}

\end{document}

% LocalWords:  CCLMs Gumperz NLP Hovy Ciot Bamman DelTredici CFB SMU
% LocalWords:  cancelled pre emptively CCLM Reddit subreddits dataset
% LocalWords:  preprocessed tokenized SpaCy tokenizer Honnibal LSTM
% LocalWords:  Hochreiter Vaswani softmax un AdamW Loshchilov IG Ppl
% LocalWords:  LM videogame embeddings PCA Kumar lstm LMCC ij th Lau
% LocalWords:  indiscernibility Vinyals Kalchbrenner OConnor
% LocalWords:  monotonicity subreddit Frobenius
