%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{latexsym}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{array}
\usepackage{cleveref}

\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.16}
\pgfplotstableset{
  column type=r,
  fixed zerofill,
  clear infinite,
  %dec sep align,
  %empty cells with={\ensuremath{-}},
}

\renewcommand{\UrlFont}{\ttfamily\small}

\newcommand\jp[1]{\todo[backgroundcolor=blue!10]{JP: #1}}
\newcommand\bn[1]{\todo[backgroundcolor=green!10]{BN: #1}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\Ppl{\mathsf{Ppl}}
\newcommand\Ind{I}
\newcommand\BibTeX{B\textsc{ib}\TeX}

\newlist{hypotheses}{enumerate}{3}
\setlist[hypotheses,1]{parsep=0pt,itemsep=1pt,font=\bfseries,label=(H\arabic*)}


\input{plot_macros}

\title{Community-Conditioned Language Models}

\author{Anon.}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}


\section{Introduction}

Linguistic communication requires that speakers share 
knowledge of certain linguistic conventions, such as syntactic structure, 
word meanings, and accepted patterns of interaction.
Speakers assume that these conventions are \emph{common ground} among their interlocutors,
based on joint membership in a community \cite{Stalnaker2002, Clark1996}.
Such a community may be of any size,
from the very small, like members of a particular friend group,
to the very large, like speakers of English.
Because it is the source of linguistic convention, the \emph{speech community} 
is an important concept in the study of linguistic variation \citep{Gumperz1972}.

\jp{We want to identify a niche that we want to fill. A quick summary of the state of the art + what is missing.}

In the present work, we investigate linguistic variation across online communities.
Broadly construed, we want to investigate two questions.  First, we
are interested in variation in the level of linguistic complexity
exhibited by different communities.

\jp{
  Some examples (perhaps include if there is space):
  
  (CFB)
  It was against SMU in the 47-48 Cotton bowl. They had cancelled a
  game the year before against Miami for the same reason. There's
  debate on if that's really the origin for the phrase but the story
  itself is true.

  (Videos)
  Reminds me of Y2K computer bug. A lot of work went into
  pre-emptively fixing it, and a lot of bugs were found and fixed,
  some of which would have been catastrophic. Unfortunately, because
  the work was successful, a lot of people now describe it as a waste
  of time/money and a big deal made out of nothing.  }



It is folklore that certain communities exhibit more linguistic complexity than others.
Here we want to quantify this phenomenon.
More precisely, we investigate if the degree of complexity can be
captured in a computational model.  For this purpose define various
Community Conditioned Language Models (CCLM for short), which can be
modeled as an estimator $M$ of the degree to which a given message $m$ 
is acceptable in a given community $c$: \(P_M(m \mid c)\).

Second, we want to find out how, and how much, CCLMs are attuned to
the specificity of communities. We test whether 1. the linguistic
models are correlated with a model of communities based only on
co-occurence of users between communities and 2. if latent community
classifiers based on the CCLM can accurately identify that a given
message comes from a given community. Additionally, one may
intuitively assume that linguistically poor communities are easier to
recognize: the less linguistic diversity, the more specific the
language become and the easier it is to identify.

To sum up we aim at testing the following hypotheses:
\begin{hypotheses}
\item \label{hyp:varying-complexity} Different communities have different levels of linguistic
  complexity
\item \label{hyp:layer-effect} The layer at which the community embedding is taken into account
  in a CCLM influences its perplexity.
\item \label{hyp:LMCC-works} CCLM can be used to recognize communities based on
  latent bayesian classification.
% \item \label{hyp:LMCC-works} The layer at which the community embedding is taken into account
%   influences CCLM and its associated latent classifier.
  % This is more of a justification for studying the latent classifier than a real "user-facing" hypothesis.
\item \label{hyp:extra-linguistic-correlation} The CCLM representation of communities are correlated with
  co-ocurrence of users between communities.
% \item Community stability is correlated with language complexity
\item \label{hyp:rich-harder-to-identify} Linguistically poor communities are easier to identify than
  linguistically rich communities.
\end{hypotheses}

\jp{Perhaps: Outline of the paper.}

\section{Experimental setup}

\subsection{Data sets}


We investigate linguistic variation across various communities present
on the social media website Reddit.\footnote{Comments were obtained
  from the archive at \url{https://pushshift.io/}.
  \cite{Baumgartner2020}.}
%
Reddit is divided into forums called \textit{subreddits}, 
which are typically organized around a topic of interest. 
Users create \textit{posts}, which consist of a link, image, 
or text, along with a \emph{comment} section. 
Comments are threaded: a comment can be made directly on a post,
or appear as a reply to another comment.
%
Hereafter we refer to such comments as ``messages'', to match our
convention in mathematical formulas: the letter $c$ stands for a
community, and $m$ stands for a message.

Our dataset includes messages from \num{46} subreddits, 
randomly selected from the set of all forums 
with at least \num{15000} messages per month for each month
in the three years between 2015 and 2017. 
We initially selected \num{50} subreddits, 
but excluded four of them from further analysis: 
two because they were primarily non-English and two with particularly short average message lengths.\footnote{
The macro-average message length was \num{39.3} ($\sigma=15.7$) and the two removed communities had average comment lengths of \num{10.7} and \num{18.1}.}
Each community corpus consist of \num{50000} randomly selected messages from the year 2015.

Messages were preprocessed as follows: 
We excluded the content of block quotes, code blocks, and tables.
We removed any markup (formatting) commands from the remaining text, extracting only rendered text.
We tokenized the messages using the default English model for the SpaCy tokenizer Version 2.2.3 \citep{Honnibal2017}
and lower-cased all tokens.

\subsection{Models}

We experiment with two kinds of model architecture: LSTM
\citep{Hochreiter1997}\jp{Is it really a simple or a bi-lstm? Even If
  it's simple it may be worth specifying due to the prevalence of
  bilstm everywhere.}  and Transformer \citep{Vaswani2017}.  In either
case, the model is organised as a standard $n$-layer neural sequence
encoder (we set $n=3$ for all models). As is standard, word tokens are
first converted to vectors using a trainable embedding layer. At the
other end, word tokens are predicted using a softmax projection layer. The set of word tokens is a
pre-determined vocabulary.  These encoders form the core of the
community-conditioned language models (CCLM), and we call them
unconditioned models.

To condition the model, we add a \emph{community embedding} parameter to the models.
This parameter is concatenated with the hidden layer of the sequence encoder,
for some layer $l_c \leq n$, and passed through a linear layer 
which projects the resulting vector back to the original hidden layer size.
For $l_c = n$, the output of this linear layer is passed directly to the softmax function,
just as the final hidden layer of the sequence encoder is in other models.
For $l_c=0$, the community embedding is concatenated with the token embedding.
For this reason, we set the hidden size of the sequence encoder 
and the size of the token to be embedding equal for all models.


\subsection{Training scheme}

We combined the 46 community corpora and split the combined corpus,
reserving 10\% for testing, 10\% for validation and the rest for training.
Splits were stratified by community so that each community had \num{5000} examples
for testing, \num{5000} for evaluation, and \num{40000} for training.

We use a vocabulary size of \num{40000} tokens, including a special out-of-vocabulary token.
The vocabulary consists of the most frequent tokens at the message level (across all communities),
meaning that we only count each token once per message.
The main reason for this choice is to avoid over-counting otherwise rare tokens that were spammed
many times in a single message.

For the LSTM, we trained the models on a simple left-to-right language modeling task
with cross-entropy loss over the vocabulary. 
Because the transformer operates on all tokens in the sequence at once,
the language modeling was achieved by masking and incrementally un-masking input tokens
in a left-to-right fashion, following \citet{BERT}\jp{Or some other standard paper}.  
We used the AdamW \citep{Loshchilov2019} optimization algorithm, 
with an initial learning rate of \num{0.001}.\jp{Also for the transformer? No simulated annealing?}
We used a batch size of \num{256} and a maximum sequence length of \num{64} tokens, 
truncating longer messages (16.8\% of messages were longer than \num{64} tokens).
During training, a dropout rate of $0.1$ was applied between encoder layers 
and after each linear layer.

All experiments use models with \num{3} sequence encoder layers,
each with hidden (and token embedding) size of \num{256}. 
The transformer models had \num{8} attention heads per layer.\footnote{
  This number of attention heads was chosen in part to give the LSTM and transformer
  models a comparable number of paramters 
  (\num{22171203} and \num{21779523}, respectively).}
The conditional models were given a community embedding with \num{16} dimensions. 
We experimented with every possible value for $l_c$ in a three-layer model ($l_c\in\{0,1,2,3\}$).

We trained the models until the validation loss stopped decreasing 
for two epochs in a row,
and used the weights from the epoch with the smallest validation loss for testing.
In general, transformer models trained for about half as long as the LSTM models (\cref{tab:best-epoch}).

\begin{table}
  \centering
  \input{floats/best_epoch.tex}
  \caption{Epoch with the lowest validation loss.}
  \label{tab:best-epoch}
\end{table}

%\pgfplotstabletypeset[
  %columns={best_epoch,lm_ppl,gain},
  %columns/model/.style={string type,column type=l},
  %columns/best_epoch/.style={column name={Epochs}, int detect},
  %columns/lm_ppl/.style={column name={Ppl}},
  %columns/gain/.style={column name={IG},precision=3,empty cells with={\ensuremath{-}}},
  %every nth row={5}{before row=\midrule},
  %]{floats/model.csv}


\section{Results and analysis}

\subsection{Losses of CCLM}

We measure the perplexity of each example in the test set. 
For this, we use the negative exponential of the mean cross-entropy loss 
for the sequence.

As one would expect, 
the conditioned models have lower perplexity
than their respective unconditioned baseline models,
but this effect is different in magnitude
for models with different community embedding depths.
For the LSTM encoder, 
the best model ($l_c=1$) concatenates the community embedding after the first encoder layer,
with two more LSTM layers after that.
The best transformer model ($l_c=3$), on the other hand,
incorporates the community information last,
after all the sequence encoder layers
(\cref{tab:model-ppls}).

%TODO: For space reasons, it might be better to put the community-level table in the appendix and have the table in body just show the means (and perhaps medians/stddevs?) for each architecture.}

We also note that the conditioned models perform differently across different communities---%
for some communities, perplexity of the conditioned models is a lot lower than the unconditioned models,
whereas for others it is the same or even slightly higher (\cref{tab:model-ppls}).\bn{talk about this in terms of mutual information instead?}\jp{What is that?}
As an informal observation, we note 
that across all the models we tested, 
communities where conditioning has the least effect
tend to be more organized around more general interest topics,
such as the \emph{relationships} subreddit, where the subject matter is relevant to a broad
range of people.
Conditioning the model on community has the most benefit
for narrower special-interest subreddits such as those organized
around a certain videogame, sports team, or subculture.
This makes sense intuitively, since communities with more niche topics
would tend to have more specialized vocabulary and other linguistic patterns.

\begin{table*}
  \footnotesize
  \centering
  \input{floats/model_ppls.tex}
  \caption{Mean model perplexity for messages in the test set, broken down by community.}
  \label{tab:model-ppls}
\end{table*}

\subsection{Information gain of the CCLM}

We consider the average information gain of each CCLM over its baseline
un-conditioned counterpart with the same sequence encoder architecture.
For a given message, the information gain is defined as the ratio of the
perplexity of the baseline model to that of the CCLM:
\[
  IG_{CCLM}(m) = \frac{Ppl_{LM}(m)}{Ppl_{CCLM}(m)}
\]

\begin{table*}
  \centering
  % \input{floats/info_gain.tex}
  \caption{Mean information gain of the CCLM over the baseline model, for messages in the test set.}
  \label{tab:model-info-gain}
\end{table*}

While the absolute performance of the LSTM models is better,
we find the the transformer CCLMs have higher information gain 
\cref{tab:model-info-gain}.

\subsection{Analysis of CCLM community embeddings}

As a qualitative assesment of the community embeddings, 
we manually identified topic categories into which at least three
of the selected communities fall.
We plotted the two-component PCA projection of the embeddings,
annotated with these categories (\cref{fig:comm-pca}).
We see that there is some evidence of clustering according to
the topic categories we identified,
suggesting that the embedding space learned by the model is meaningful.

\begin{figure*}
\begin{tikzpicture}
  \commpcagroupplot{floats/lstm-3-1_pca.csv}{floats/transformer-3-3_pca.csv}
\end{tikzpicture}
\caption{Community embedding PCA of the best LSTM (left, $c=1$) and transformer (right, $c=3$) models.}
\label{fig:comm-pca}
\end{figure*}

As a quantitative assessment, we compare the CCLM-learned community
embedding with the community embedding created by \citet{Kumar2018},
who generated them from the message-community co-occurence matrix,
using data which spans January 2014 to April 2017. We refer the reader
to \citet{Kumar2018}\jp{I actually cannot find in their paper what
  they do.} for details, but the important point is that no linguistic
information is used to create these embeddings: they only depend on
whether and how much a given user participates in a given
community. In sharp contrasts, CCLM community embeddings depend in no
way on which user is the author of any given message: we only use the
contents of messages, not authorship data. 
%
We correlate the pairwise cosine similarity of the
CCLM embeddings with the same metric on the membership embedding,
restricted to the 46 communities chosen for our study
(\cref{fig:pairwise-comm-sim}).

\begin{figure}
  \pairwisecommsim{lstm-3-1} 
  \caption{Pairwise cosine similarity between community vectors for 
    the membership-based embedding (x-axis) and
  the LSTM CCLM embedding with $c=1$ (y-axis). 
  Pairs from the same category are colored according to \cref{fig:comm-pca}.
  We also mark pairs from different (\texttimes) and pairs where one or both
  communities belong to the ``Other'' category (\textbigcircle).
  }
  \label{fig:pairwise-comm-sim}
\end{figure}

\begin{table}
  \centering
  \input{floats/comm_sim.tex}
  \caption{Pearson's r between the pairwise similarity of community vectors
  in the CCLM models and the membership-based embedding of \citet{Kumar2018}.
  ($p<0.001$ for all models).
  }
  \label{tab:pairwise-comm-sim}
\end{table}

\subsection{Latent language-model-based community classifiers}

The perplexity of a CCLM gives us a measure of linguistic complexity,
but it is not a measure of how linguistically \emph{specific} the
community is. However, the CCLM induces a latent classifier,
which we call a language model-based community classifier
(LMCC). Using the Bayes theorem, we can turn a
CCLM into a LMCC: the probability that a given message $m$
belongs to a community $c_i$ is calculated as follows:
\[P(c=c_i | m) = P(m | c_i)\frac {P(m)} {P(c=c_i)}\]
In the above,
$P(c=c_i)$ is the frequency of community $c_i$ in the dataset, and
$P(m | c_i)$ (the probability that a message $m$
belongs in a community $c_i$. ) is estimated by the CCLM. $P(m)$ is the absolute
probability of a message, which can be computed as
\[P(m) = \frac 1 {\sum_i P(c=c_i) P(m|c=c_i ) }\].

We can characterise the quality the LMCC with a confusion matrix
$C$, defined by $C_{ij} = P(c=c_i | m)$ for a given (test set) message $m$ 
from community $c_j$. 
That is: \[C_{ij} = average_{m \in Messages(c_j)}(P(c=c_i | m))\].
As a sanity check, we observe that the diagonal dominates the matrix: on average the LMCC picks the right community for a message (\cref{fig:confusion}).

\begin{figure*}
\begin{tikzpicture}
  \confusiongroupplot{lstm-3-1}{transformer-3-3}
\end{tikzpicture}
\caption{LMCC confusion matrices for the best LSTM (left, $c=1$) and transformer (right, $c=3$) models.}
\label{fig:confusion}
\end{figure*}

However, we want to measure the ability of the LMCC to correctly
recognize messages coming from a given community $c_j$. We could use
something like an F-score, but we prefer to use the entropy metric,
which is has stronger information-theoretic grounds, and is a measure
of the uncertainty of the LMCC, given messages from a particular 
community. 
Technically, the linguistic indiscernibility of community $c_j$ is defined 
as the exponential of the entropy of the $j$ row in \(C\),
which is a probability distribution 
of predicted community for a random message from $c_j$.
In sum, we have the formula: \bn{Took out $D$. is it needed elsewhere?}
\[\Ind_j = e^{-\sum_i C_{ij} log(C_{ij})}\]

Indiscernibility is a robust measure: it is almost perfectly
correlated for various CCLM (transformer, LSTM and various $c$) $M$.
In fact, we observe a correlation between the indiscernibility 
measures by every pair of models with Pearson's r $\geq 0.98$ 
and $p < 0.0001$ (see, for example, \cref{fig:lmcc_ppl}).

\begin{figure}
  \pplscatter{lstm-3-1_lmcc}{transformer-3-3_lmcc}
  \caption{%
    Correlation between indiscernibility for the best LSTM ($c = 1$) and transformer ($c = 3$) models
    (Pearson's $r = 0.99$, $p < 0.0001$).
  }
  \label{fig:lmcc_ppl}
\end{figure}

However, we observe that community-wise CCLM perplexity is only weakly (if at all) \bn{Changed the indiscernability (y-axis) from entropy to Ppl and the p-values all shot up above 0.1. However the r is still very weakly positive for every model.}
correlated with community indiscernibility under the corresponding latent 
LMCC (see \cref{fig:cclm_lmcc_ppl} and \cref{tab:cclm_lmcc_ppl}).
In other words, the linguistic complexity of a given community
is a poor predictor of how distinct its language is.

\jp{actually, if we had many more communities, then the lines could be ``more blurred'' between various communities, and we may have a stronger correlation here.}
\begin{figure}
  \pplscatter{lstm-3-1_cclm}{lstm-3-1_lmcc}
  \caption{%
    Mean community-wise linguistic perplexity (x-axis) 
    and indiscernibility (y-axis) 
    (Pearson's r = \num{0.16}; p = \num{0.2991}).  
    Shown here for the LSTM with $c = 1$.}
  \label{fig:cclm_lmcc_ppl}
\end{figure}

\begin{table}
  \centering
  \input{floats/cclm_lmcc_ppl.tex}
  \caption{
  Pearson's r correlation between community-wise linguistic perplexity and indescernibility for each model.
  }
  \label{tab:cclm_lmcc_ppl}
\end{table}

\jp{We could also compute the entropy of rows of Cij = snap cosine similarity w/ softmax.}

On the other hand, community indiscernibility is correlated with
the average information gain of the CCLM over the baseline model,
for messages in its test set \cref{fig:comm-pca}.
Communities for which the language model bennifits most from 
the conditioned model are generally easier for the LMCC to classify.

\begin{figure}
  \pplscatter{lstm-3-1_gain}{lstm-3-1_lmcc}
  \caption{%
    Mean community-wise information gain (x-axis) 
    and indiscernibility (y-axis) 
    %(Pearson's r = \num{0.16}; p = \num{0.2991}).  
    Shown here for the LSTM with $c = 1$.}
  \label{fig:cclm_lmcc_ppl}
\end{figure}

\begin{table}
  \centering
  % \input{floats/lmcc_gain.tex}
  \caption{
    Pearson's r correlation between community-wise information gain (of the CCLM over the un-conditioned model) and indescernibility.
  }
  \label{tab:lmcc_gain}
\end{table}


% Finally, we investigate the correlation between the stability of a
% community and the perplexity of the LMCC for it. Stability is defined as TODO.

% - compute pearson correlation coefficient

% We see a positive correlation: this suggests that the more stable a
% community is, the more difficult it is to identify message as coming
% from it. What this suggests is that stable communities tend to use a
% varied and standard, subset of English, while unstable communities
% resort to more formulaic language. This may suggests that to 'fit' in an
% unstable community, one must use more obvious linguistic cues than in
% a stable community. In fact, it is reasonable to think that a new
% member will ostensibly use community-specifc language, but once
% established in a community, a member would tend not to do so.

\section{Related work}

In this work, we present results using a conditioned neural language models
to model variation between speech communities.
The architecture of these models concatenates a vector representation
of the conditioned variable to the input of the sequence model.
This approach has been applied in various conditioned text generation domains such as 
image captioning \citep{Vinyals2015}, machine translation \citep{Kalchbrenner2013},
but it has not, to our knowledeg, been used extensively to study linguistic variation.
There are, however some related methods that have been used to study variation,
and related applications of conditional neural language models.

In the later category, conditoinal nerual language models have been used to study linguistic change.

\paragraph{Vector representations of online communities encode semantic relationships}
https://www.aclweb.org/anthology/W17-2904.pdf

\paragraph{Reddit Mining to Understand Gendered Movements}
http://ceur-ws.org/Vol-2578/DARLIAP3.pdf

\paragraph{Language models for sociolinguistic variation}
\cite{DelTredici2017}

\paragraph{Topic modelling}
- Discovering Discrete Latent Topics with Neural Variational Inference https://arxiv.org/pdf/1706.00359.pdf
- JeyHan Lau's work

\paragraph{Pre-trained language models}
- Unsupervised Domain Clusters in Pretrained Language Models https://www.aclweb.org/anthology/2020.acl-main.692/


\section{Discussion and Conclusion}


We find that various communities exhibit widely varying levels of
linguistic complexity \ref{hyp:varying-complexity}, with perplexity
per-word ranging from less than 53 to 104 for the best model
architecture.

With indiscernibility values of 1.5 to 3.5 (out of a maximum of 46),
we find that CCLMs can be used to recognize communities based on
latent bayesian classification \ref{hyp:LMCC-works}. Even the most
generic communities appear to exhibit fairly specific linguistic
patterns. Furthermore, these patterns are just as recognizable by LSTM
or Transformer models, when properly configured and trained.

We find a weak correlation (\cref{tab:cclm_lmcc_ppl}) between the perplexity of the \bn{I'm not sure if we can make this claim or not with the Ppl-Ppl correlations. However, given that *all* of them have positive correlation, even though the p-values are somewhat higher than .1 maybe we can?}
CCLM and the indiscernibility of the corresponding community
\ref{hyp:rich-harder-to-identify}. Because we have relatively few,
and only large communities in our test set, it is difficult to
provide a conclusive answer, but this kind of correlation would suggest that
community-specific complexity can be partially attributed to using
more varied patterns. However, even the most complex communities
(perplexity per word above 100) have an indiscernibility of about 3
(out of a maximum of 46), so they retain fairly specific patterns:
they are not simply an even mixture of the kind of comments found in
other communities.

We find that the layer at which the community embedding is taken into
account in a CCLM influences its perplexity \ref{hyp:layer-effect},
albeit weakly. For LSTM models, the perplexity per word, averaged over
all communities, varies between 76.80 and 78.64. (with 80.70 for the unconditioned model)
The perplexity per word is slightly higher for $l_c=0$, and has very
little difference between the other models, with $l_c=1$ being the best.
For transformer models, it varies
between 89.38 and 122.56. Here perplexity per-word is clearly not a monotonous
function of $l_c$: the maximum is reached at $l_c=2$. We did not
expect non-mononicity, and leave studying this effect to future work.

The pattern of information gain by community is similar across architectures.
Communities that bennifit most from the conditioned model behave that way for 
both the LSTM and transformer.
However, there are some differences.
For example, the subreddit \emph{relationships} has an information gain of \num{1.018} for
the best CCLM LSTM but only \num{0.998} for the best transformer model.
Why was the LSTM able to take advantage of the community information for
this community, where the transformer wasn't?
It would be interesting to investigate these differences in future work,
since it could reveal differences in the kind of linguistic variation the different
model architectures capture.

The representation of communities built by CCLM exhibits a clear
correlation with user-coocurrence patterns
(\ref{hyp:extra-linguistic-correlation}). A natural
question that may arise is how much this correlation can be explain by
user-specifc linguistic patterns. We plan to control for this variable
in future work, for example by building author-conditioned language
models and testing if community vector embeddings can be determined by
the set of the vector embeddings of its members.

\bibliography{paper}
\bibliographystyle{acl_natbib}

\end{document}
