%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{latexsym}
\usepackage{siunitx}
\usepackage{booktabs}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Community-Conditioned Language Models}

\author{Anon.}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}


\section{Introduction}

We propose to test the following hypotheses:

\begin{enumerate}
\item Different communities have different levels of linguistic
  complexity
\item different language models are able to recognize such differences
\item level at which the community embedding is taken into account in the language model influences this.
\item community vectors are correlated with extra-linguistic information.
\item stability is correlated with language complexity
\end{enumerate}

\section{Experimental setup}

\subsection{Data sets}

To investigate variation across communities, we use comments collected 
from the social media website Reddit.\footnote{Comments were obtained 
from the archive at \href{https://pushshift.io/}{pushshift.io} \cite{Baumgartner2020}.}

Reddit is divided into forums called \textit{subreddits}, 
which are typically organized around a topic of interest. 
Our dataset includes comments from \num{46} subreddits, 
randomly selected from the set of all forums 
with at least \num{15000} comments per month for each month 
in the three years between 2015 and 2017.\footnote{
We initially selected \num{50} subreddits, 
but excluded four from further analysis: 
two which were primarily non-English and two with particularly short average comment lengths.}
Each community corpus consist of \num{50000} randomly selected comments from the year 2015.

Comments were preprocessed as follows: 
We removed markdown formatting, extracting only rendered text. 
We exclude the content of block quotes, code blocks, and tables. 
We tokenized the comments using the default English model for the SpaCy tokenizer,\footnote{Version 2.2.3}
and lower-cased all tokens. 

\subsection{Models}

At the the core of the community-conditioned language model (CCLM) 
is a standard $n$-layer neural sequence encoder 
with token embedding (we experiment with LSTM and transformer encoders).
To condition the model, we additionally train a \emph{community embedding},
which is concatenated with the hidden layer of the sequence encoder
for some layer $l_c \leq n$ and passed through a linear layer, 
which projects the resulting vector back down to the origial hidden size.
For $l_c=0$, the community embedding is concatenated directly to the token embedding.
For this reason, the hidden size of the sequence encoder 
and the size of the token embedding are always the same.

\subsection{Training scheme}

We combined the 46 community corproa and split the combined corpus,
reserving 10\% for testing, 10\% for validation and the rest for training.
Splits were stratified by community so that the same number of training examples
were available for each community.

We used a vocabulary size of \num{40000} tokens, with a special out-of-vocabulary token.
The vocabulary consisted of the most frequent tokens at the comment level,
meaning that we only counted each token once per comment.
The main reason for this was to avoid over-counting otherwise rare tokens that were spammed
many times in a single comment.

We trained the models on a simple left-to-right language modeling task 
with cross entropy loss over the vocabulary 
and AdamW \citep{Loshchilov2019} optimization, with an inital learning rate of \num{0.001}.
We used a batch size of \num{256} and a maximum sequence length of \num{64} tokens.
During training, a dropout rate of $0.1$ was applied between encoder layers 
and after each linear layer.

All experiments used models with \num{3} sequence encoder layers, 
each with hidden (and token embedding) size of \num{256}. 
The transformer models had \num{8} attention heads per layer.\footnote{
  This number of attention heads was chosen to give the LSTM and transformer
  models a comparable number of paramters 
  (\num{22171203} and \num{21779523}, respectively).}
The conditional models were given a community embedding with \num{16} dimensions. 

We trained the models until the validation loss was smaller than the smallest
validation loss for two epochs in a row. 
The weights from the epoch with the smallest validation loss were used for testing.

\begin{table}
  \centering
  \input{floats/best_epoch.tex}
  \label{tab:best-epoch}
  \caption{Epoch with the lowes validation loss.}
\end{table}

\section{Results and analysis}

\subsection{Losses of CCLM}

\begin{table*}
  \footnotesize
  \centering
  \input{floats/model_ppls.tex}
  \label{tab:model-ppls}
  \caption{Mean model test perplexity.}
\end{table*}

% Computation of loss/community/model (show all)
\begin{table}
  \centering
  \begin{tabular}{ccc}
    todo &todo &todo 
  \end{tabular}
  \caption{Model losses per community}
  \label{tab:losses}
\end{table}
TODO commentary. Why are some communities low/high.

\subsection{Analysis of community embeddings}

- PCA plots: not conclusive:

- Correlation with co-occurence matrix (not conclusive)

- Analysis: probably because the model is not using dimensions
with \emph{a priori} equal variance in all dimensions. (Both the LSTM and transformer are highly non-linear complex models)

\subsection{Implicit language-model-based community classifiers}

(TODO: Insert a the correlation plot between two models, on the exponential scale so that it's the same as the other later plots)
TODO: So we can't use the losses directly. Why is this so?


However, we are not so much interested in the ability of the CCLM to
precisely model the language used in a community.

We now turn our interest to language-model-based community classifiers
(LMCC). Given a post $m$, and LMCC gives the probability that it
belongs in a community $c_i$ Using the Bayes theorem, we can turn a
CCLM into a LMCC. Indeed, the probability that a given post $m$
belongs to a community $c_i$ can be calculated as follows:
\[P(c=c_i | m) = P(m | c_i)\frac {P(m)} {P(c=c_i)}\]
In the above,
$P(c=c_i)$ is the frequency of community $c_i$ in the dataset, and
$P(m | c_i)$ is given by the CCLM. $P(m)$ is our normalisation factor, which can be set so that
\[\sum_i P(c=c_i | m) = 1\].

Knowing that $m$ \emph{actually} comes from community $c_j$, we can
measure the the precision our classifier, as a confusion matrix $C$,
which can be calculated as follows for a given set of posts:(TODO: what set exactly?)
\[C_{ij} = average_{m âˆˆ Posts(c_j)}(P(c=c_i | m))\].

TODO: show a few relevant matrices. (eg. show only best and worst model) and commentary.

TODO: why don't we use F scores?

While the confusion matrices provide a lot of insight, they are
somewhat \emph{too} fine-grained. For this reason, we will be
interested in confusing the posts from community $c_j$ are to the
LMCC. For this purpose, we consider the perplexity of the LMCC for a
post coming from community $j$, defined as the exponential of the
entropy of the distribution \(D_j\), which is the (discrete)
probability distribution of predicted community for a random post from
$c_j$. $D_j$ is simply the $j$th row in \(C\), and so we obtain the formula:
\[Ppl_j = e^{-\sum_i C_{ij} log(C_{ij})}\]

To recap, \(Ppl_j\) is a measure of the capability (or rather
inability) of the model to correctly identify a message as coming from
a certain community $c_j$. 

We can then analyse the correlation between \(Ppl^M_j\) for various models $M$.

TODO: show a scatter plot, compute r2, analyse out-of-balance points; discuss why they stand where they do.


Finally, we investigate the correlation between the stability of a
community and the perplexity of the LMCC for it. Stability is defined as TODO.

- computer r2.

We see a positive correlation: this suggests that the more stable a
community is, the more difficult it is to identify posts as coming
from it. What this suggests is that stable communities tend to use a
varied and standard, subset of English, while unstable communities
resort to more formulaic posts. This may suggests that to 'fit' in an
unstable community, one must use more obvious linguistic cues than in
a stable community. In fact, it is reasonable to think that a new
member will ostensibly use community-specifc language, but once
established in a community, a member would tend not to do so.

\section{Discussion and Conclusion}

- Comparison with topic modelling

\bibliography{paper}
\bibliographystyle{acl_natbib}

\end{document}
