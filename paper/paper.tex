%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{latexsym}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{todonotes}
\usepackage{enumitem}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.16}


\newcommand\jp[1]{\todo[backgroundcolor=blue!10]{JP: #1}}
\newcommand\bn[1]{\todo[backgroundcolor=green!10]{BN: #1}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\newlist{hypotheses}{enumerate}{3}
\setlist[hypotheses,1]{parsep=0pt,itemsep=1pt,font=\bfseries,label=(H\arabic*)}


\input{plot_macros}

\title{Community-Conditioned Language Models}

\author{Anon.}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}


\section{Introduction}


TODO: What is a linguistic community. 

Broadly construed, we want to investigate two questions.  First, we
are interested in the variation of to what extent are various
communities exhibiting various level of linguistic complexity.

TODO: insert some anecdotes (perhaps some particularly stupid/clever
comments from reddit here.)

It is folklore that certain communities are more complex
linguistically: here we want to quantify this phenomenon.

More precisely, we investigate if this degree of complexity be
captured in a computational model.  For this purpose define various
Community Conditioned Language Models (CCLM for short), which can be
modeled as an estimator $M$ what a given message $m$ is acceptable in
a given community $c$: \(P_M(m \mid c)\).

Second, we want to find out how, and how much, CCLMs can be attuned to
the specificity of communities. We test whether 1. the linguistic
models are correlated with a model of community based only on
co-occurence of users between communities and 2. if latent community
classifiers based on the CCLM can accurately identify that a given message
comes from a given community.

To sum up, we propose to test the following hypotheses:
\begin{hypotheses}
\item Different communities have different levels of linguistic
  complexity
\item The layer at which the community embedding is taken into account
  in CCLM influences its perplexity.
\item CCLM can be used to recognize communities based on
  latent bayesian classification.
\item The layer at which the community embedding is taken into account
  influences CCLM and its associated latent classifier.
  % \item There is a correlation between community language complexity and recognizability.
  % This is more of a justification for studying the latent classifier than a real "user-facing" hypothesis.
\item The model representation of communities are correlated with
  co-ocurrence of users between communities.
% \item Community stability is correlated with language complexity
\end{hypotheses}

\section{Experimental setup}

\subsection{Data sets}

To investigate variation across communities, we use comments collected
from the social media website Reddit.\footnote{Comments were obtained
  from the archive at \href{https://pushshift.io/}{pushshift.io}
  \cite{Baumgartner2020}.} 

Reddit is divided into forums called \textit{subreddits}, 
which are typically organized around a topic of interest. 
Users create \textit{posts}, which consist of a link, image, 
or text, along with a \emph{comment} section. 
Comments are threaded: a comment can be made directly on a post,
or appear as a reply to another comment.
%
Hereafter we refer to such comments as ``messages'', to match our
convention in mathematical formulas: the letter $c$ stands for a
community, and $m$ stands for a message.

Our dataset includes messages from \num{46} subreddits, 
randomly selected from the set of all forums 
with at least \num{15000} messages per month for each month
in the three years between 2015 and 2017. 
We initially selected \num{50} subreddits, 
but excluded four of them from further analysis: 
two because they were primarily non-English and two with particularly\jp{quantify?} short average message lengths.
Each community corpus consist of \num{50000} randomly selected messages from the year 2015.

Messages were preprocessed as follows: 
We excluded the content of block quotes, code blocks, and tables.
We removed any markup (formatting) commands from the remaining text, extracting only rendered text.
We tokenized the messages using the default English model for the SpaCy tokenizer Version 2.2.3 \citep{Honnibal2017}
and lower-cased all tokens.

\subsection{Models}

We experiment with two kinds of architectures for our models: an LSTM
and a Transformer architecture\jp{citations}.  In either case, they
are organised as a standard $n$-layer neural sequence encoder with
token embedding (we set $n=3$ for all models). As standard, words are predicted
using a softmax projection layer.
Such encoders form the
core of the community-conditioned language models (CCLM).

To condition the model, we add a \emph{community embedding} parameter to the models.
This parameter is concatenated with the hidden layer of the sequence encoder,
for some layer $c \leq n$, and passed through a linear layer 
which projects the resulting vector back to the origial hidden layer size.
For $c = n$, the output of this linear layer is passed directly to the softmax function,
just as the final hidden layer of the sequence encoder is in other models.
For $c=0$, the community embedding is concatenated with the token embedding.
For this reason, we set the hidden size of the sequence encoder 
and the size of the token to be embedding equal for all models. 

\subsection{Training scheme}

We combined the 46 community corpora and split the combined corpus,
reserving 10\% for testing, 10\% for validation and the rest for training.
Splits were stratified by community so that the same number of training examples
were available for each community.\jp{is it the same for testing/validation? How do you maintain this split?}

We use a vocabulary size of \num{40000} tokens, including a special out-of-vocabulary token.
The vocabulary consists of the most frequent tokens at the message level,
meaning that we only count each token once per message.
The main reason for this choice was to avoid over-counting otherwise rare tokens that were spammed
many times in a single message.

We trained the models on a simple left-to-right\jp{I don't think that this applies for the transformer model} language modeling task 
with cross entropy loss over the vocabulary 
and AdamW \citep{Loshchilov2019} optimization, with an inital learning rate of \num{0.001}.
We used a batch size of \num{256} and a maximum sequence length of \num{64} tokens, truncating longer messages (16.8\% of messages were longer than \num{64} tokens).
During training, a dropout rate of $0.1$ was applied between encoder layers 
and after each linear layer.

All experiments use models with \num{3} sequence encoder layers,
each with hidden (and token embedding) size of \num{256}. 
The transformer models had \num{8} attention heads per layer.\footnote{
  This number of attention heads was chosen in part to give the LSTM and transformer
  models a comparable number of paramters 
  (\num{22171203} and \num{21779523}, respectively).}
The conditional models were given a community embedding with \num{16} dimensions. 

We trained the models until the validation loss stopped decreasing 
for two epochs in a row,
and used the weights from the epoch with the smallest validation loss for testing.
In general, transformer models trained for about half as long as the LSTM models (Table~\ref{tab:best-epoch}).

\begin{table}
  \centering
  \input{floats/best_epoch.tex}
  \caption{Epoch with the lowest validation loss.}
  \label{tab:best-epoch}
\end{table}

\section{Results and analysis}

\subsection{Losses of CCLM}

As one would expect, 
the conditioned models have lower perplexity 
than their respective unconditioned baseline models,
but this effect is different in magnitude
for models with different community embedding depths .
For the LSTM encoder, 
the best model ($c=1$) concatenates the community embedding after the first encoder layer,
with two more LSTM layers after that.
The best transformer model ($c=3$), on the other hand,
incorporates the community information last,
after all the sequence encoder layers
(Table \ref{tab:model-ppls}).

%TODO: For space reasons, it might be better to put the community-level table in the appendix and have the table in body just show the means (and perhaps medians/stddevs?) for each architecture.}

We also note that the conditioned models perform differently across different communities---\
for some communities, the test perplexity of the conditioned models is a lot lower (Table~\ref{tab:model-ppls},
whereas for others it is the same or even slightly higher.
\bn{TODO: talk about this in terms of mutual information instead?}
As an anecdotal observation, we note 
that communities where conditioning has the least effect
tend to be more organized around more ``general interest'' topics.\jp{Is there a way to quantify general interest topics? Community size in terms of number of users?}
This makes sense intuitively, since communities with more niche topics
would tend to have more specialized vocabulary and speech patterns.

The pattern of how much the perplexity falls across different communities
is similar across architectures, but with some differences.
It could be interesting to further investigate these differences in future work,
since it could reveal differences in the kind of linguistic variation the different
model architectures capture.

\begin{table*}
  \footnotesize
  \centering
  \input{floats/model_ppls.tex}
  \caption{Mean model test perplexity per community.}
  \label{tab:model-ppls}
\end{table*}

\subsection{Analysis of CCLM community embeddings}

As a qualitative assesment of the community embeddings, 
we manually identified topic categories into which at least three
of the selected communities fall. 
We plotted the two-component PCA projection of the embeddings,
annotated with these categories (Figure~\ref{fig:comm-pca}).
We see that there is some evidence of clustering according to
the topic categories we identified,
suggesting that the embedding space learned by the model is meaningful.

\begin{figure*}
\begin{tikzpicture}
  \commpcagroupplot{floats/lstm-3-1_pca.csv}{floats/transformer-3-3_pca.csv}
\end{tikzpicture}
\caption{Community embedding PCA of the best LSTM (left, $c=1$) and transformer (right, $c=3$) models.}
\label{fig:comm-pca}
\end{figure*}

As a quantitative assessment,
we compare the CCLM-learned community embedding with the community embedding from \citet{Kumar2018},
which is based on community membership. 

- Description of the snap data (JP)

We correlate the pairwise cosine similarity of the
CCLM embeddings with the same metric on the membership embedding,
restricted to the 46 communities chosen for our study
(Figure~\ref{fig:pairwise-comm-sim}).

\begin{figure}
  \pairwisecommsim{lstm-3-1} 
  \caption{Pairwise cosine similarity between community vectors for 
    the membership-based embedding (x-axis) and
  the LSTM CCLM embedding with $c=1$ (y-axis). 
  Pairs from the same category are colored according to Figure~\ref{fig:comm-pca}.
  We also mark pairs from different (\texttimes) and pairs where one or both
  communities belong to the ``Other'' category (\textbigcircle).
  }
  \label{fig:pairwise-comm-sim}
\end{figure}

\begin{table}
  \centering
  \input{floats/comm_sim.tex}
  \caption{Pearson's r between the pairwise similarity of community vectors
  in the CCLM models and the membership-based embedding of \citet{Kumar2018}.
  ($p<0.001$ for all models).
  }
  \label{tab:pairwise-comm-sim}
\end{table}

\subsection{Latent language-model-based community classifiers}

- Two different use of the model
1. ``absolute'' perplexity
2. latent ability to recognize a community


However, we are not so much interested in the ability of the CCLM to
precisely model the language used in a community.

We now turn our interest to language-model-based community classifiers
(LMCC). Given a message $m$, and LMCC gives the probability that it
belongs in a community $c_i$ Using the Bayes theorem, we can turn a
CCLM into a LMCC. Indeed, the probability that a given message $m$
belongs to a community $c_i$ can be calculated as follows:
\[P(c=c_i | m) = P(m | c_i)\frac {P(m)} {P(c=c_i)}\]
In the above,
$P(c=c_i)$ is the frequency of community $c_i$ in the dataset, and
$P(m | c_i)$ is given by the CCLM. $P(m)$ is our normalisation factor, which can be set so that
\[\sum_i P(c=c_i | m) = 1\].

Knowing that $m$ \emph{actually} comes from community $c_j$, we can
measure the precision our classifier, as a confusion matrix $C$,
which can be calculated as follows for a given set of messages (TODO: what set exactly?)
\[C_{ij} = average_{m \in Messages(c_j)}(P(c=c_i | m))\].


%\begin{figure*}
%\begin{tikzpicture}
  %\confusiongroupplot{floats/lstm-3-1_comm_infer_confusion.csv}{floats/transformer-3-3_comm_infer_confusion.csv}
%\end{tikzpicture}
%\caption{LMCC confusion matrices for the best LSTM (left, $c=1$) and transformer (right, $c=3$) models.}
%\label{fig:confusion}
%\end{figure*}


TODO: why don't we use F scores?

While the confusion matrices provide a lot of insight, they are
somewhat \emph{too} fine-grained. For this reason, we will be
interested in confusing the messages from community $c_j$ are to the
LMCC. For this purpose, we consider the perplexity of the LMCC for a
message coming from community $j$, defined as the exponential of the
entropy of the distribution \(D_j\), which is the (discrete)
probability distribution of predicted community for a random message from
$c_j$. $D_j$ is simply the $j$th row in \(C\), and so we obtain the formula:
\[Ppl_j = e^{-\sum_i C_{ij} log(C_{ij})}\]

To recap, \(Ppl_j\) is a measure of the capability (or rather
inability) of the model to correctly identify a message as coming from
a certain community $c_j$. 

  %\begin{tikzpicture}
  %\pplscatter{lstm-3-1_lmcc}{lstm-3-1_cclm}
  %\end{tikzpicture}

\begin{figure}
  \pplscatter{lstm-3-1_cclm}{lstm-3-1_lmcc}
  \caption{%
    Mean CCLM (x-axis) and LMCC (y-axis) perplexity per community (Pearson's r = \num{0.30}, p = \num{0.0444}).
    Shown here for the LSTM model with $c = 1$, but other models exhibit similar behavior.
  }
  \label{fig:cclm_lmcc_ppl}
\end{figure}


We can then analyse the correlation between \(Ppl^M_j\) for various models $M$.

\begin{figure}
  \pplscatter{lstm-3-1_lmcc}{transformer-3-3_lmcc}
  \caption{%
    Mean CCLM perplexity per community for the best LSTM ($c = 1$) and transformer ($c = 3$) models
    (Pearson's r = \num{0.99} p < \num{0.0001}).
  }
  \label{fig:lmcc_ppl}
\end{figure}


% Finally, we investigate the correlation between the stability of a
% community and the perplexity of the LMCC for it. Stability is defined as TODO.

% - compute pearson correlation coefficient

% We see a positive correlation: this suggests that the more stable a
% community is, the more difficult it is to identify message as coming
% from it. What this suggests is that stable communities tend to use a
% varied and standard, subset of English, while unstable communities
% resort to more formulaic language. This may suggests that to 'fit' in an
% unstable community, one must use more obvious linguistic cues than in
% a stable community. In fact, it is reasonable to think that a new
% member will ostensibly use community-specifc language, but once
% established in a community, a member would tend not to do so.

\section{Related work}

\paragraph{Vector representations of online communities encode semantic relationships}
https://www.aclweb.org/anthology/W17-2904.pdf

\paragraph{Reddit Mining to Understand Gendered Movements}
http://ceur-ws.org/Vol-2578/DARLIAP3.pdf

\paragraph{Language models for sociolinguistic variation}
\cite{DelTredici2017}

\paragraph{Topic modelling}
- Discovering Discrete Latent Topics with Neural Variational Inference https://arxiv.org/pdf/1706.00359.pdf
- JeyHan Lau's work

\paragraph{Pre-trained language models}
- Unsupervised Domain Clusters in Pretrained Language Models https://www.aclweb.org/anthology/2020.acl-main.692/


\section{Discussion and Conclusion}

- Comparison with topic modelling

\bibliography{paper}
\bibliographystyle{acl_natbib}

\end{document}
