%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Community-Conditioned Language Models}

\author{Anon.}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}


\section{Introduction}

We propose to test the following hypotheses:

\begin{enumerate}
\item Different communities have different levels of linguistic
  complexity
\item different language models are able to recognize such differences
\item level at which the community embedding is taken into account in the language model influences this.
\item community vectors are correlated with extra-linguistic information.
\item stability is correlated with language complexity
\end{enumerate}

\section{Experimental setup}
\subsection{Data Sets}
\subsection{Models}
\subsection{Training scheme}

\section{Results and analysis}

\subsection{Losses of CCLM}

% Computation of loss/community/model (show all)
\begin{table}
  \centering
  \begin{tabular}{ccc}
    todo &todo &todo 
  \end{tabular}
  \caption{Model losses per community}
  \label{tab:losses}
\end{table}
TODO commentary. Why are some communities low/high.

\subsection{Analysis of community embeddings}

- PCA plots: not conclusive:

- Correlation with co-occurence matrix (not conclusive)

- Analysis: probably because the model is not using dimensions
with \emph{a priori} equal variance in all dimensions. (Both the LSTM and transformer are highly non-linear complex models)

\subsection{Implicit language-model-based community classifiers}

(TODO: Insert a the correlation plot between two models, on the exponential scale so that it's the same as the other later plots)
TODO: So we can't use the losses directly. Why is this so?


However, we are not so much interested in the ability of the CCLM to
precisely model the language used in a community.

We now turn our interest to language-model-based community classifiers
(LMCC). Given a post $m$, and LMCC gives the probability that it
belongs in a community $c_i$ Using the Bayes theorem, we can turn a
CCLM into a LMCC. Indeed, the probability that a given post $m$
belongs to a community $c_i$ can be calculated as follows:
\[P(c=c_i | m) = P(m | c_i)\frac {P(m)} {P(c=c_i)}\]
In the above,
$P(c=c_i)$ is the frequency of community $c_i$ in the dataset, and
$P(m | c_i)$ is given by the CCLM. $P(m)$ is our normalisation factor, which can be set so that
\[\sum_i P(c=c_i | m) = 1\].

Knowing that $m$ \emph{actually} comes from community $c_j$, we can
measure the the precision our classifier, as a confusion matrix $C$,
which can be calculated as follows for a given set of posts:(TODO: what set exactly?)
\[C_{ij} = average_{m âˆˆ Posts(c_j)}(P(c=c_i | m))\].

TODO: show a few relevant matrices. (eg. show only best and worst model) and commentary.

TODO: why don't we use F scores?

While the confusion matrices provide a lot of insight, they are
somewhat \emph{too} fine-grained. For this reason, we will be
interested in confusing the posts from community $c_j$ are to the
LMCC. For this purpose, we consider the perplexity of the LMCC for a
post coming from community $j$, defined as the exponential of the
entropy of the distribution \(D_j\), which is the (discrete)
probability distribution of predicted community for a random post from
$c_j$. $D_j$ is simply the $j$th row in \(C\), and so we obtain the formula:
\[Ppl_j = e^{-\sum_i C_{ij} log(C_{ij})}\]

To recap, \(Ppl_j\) is a measure of the capability (or rather
inability) of the model to correctly identify a message as coming from
a certain community $c_j$. 

We can then analyse the correlation between \(Ppl^M_j\) for various models $M$.

TODO: show a scatter plot, compute r2, analyse out-of-balance points; discuss why they stand where they do.


Finally, we investigate the correlation between the stability of a
community and the perplexity of the LMCC for it. Stability is defined as TODO.

- computer r2.

We see a positive correlation: this suggests that the more stable a
community is, the more difficult it is to identify posts as coming
from it. What this suggests is that stable communities tend to use a
varied and standard, subset of English, while unstable communities
resort to more formulaic posts. This may suggests that to 'fit' in an
unstable community, one must use more obvious linguistic cues than in
a stable community. In fact, it is reasonable to think that a new
member will ostensibly use community-specifc language, but once
established in a community, a member would tend not to do so.

\section{Discussion and Conclusion}

- Comparison with topic modelling

\end{document}
