
@article{Bamman2014,
  title = {Gender Identity and Lexical Variation in Social Media},
  author = {Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler},
  year = {2014},
  volume = {18},
  pages = {135--160},
  issn = {1467-9841},
  doi = {10.1111/josl.12080},
  abstract = {We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/josl.12080},
  copyright = {\textcopyright{} 2014 John Wiley \& Sons Ltd},
  journal = {Journal of Sociolinguistics},
  keywords = {computational methods,computer-mediated communication,Gender,social media,social networks,style},
  language = {en},
  number = {2}
}

@inproceedings{Bamman2014a,
  title = {Distributed {{Representations}} of {{Geographically Situated Language}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Bamman, David and Dyer, Chris and Smith, Noah A.},
  year = {2014},
  month = jun,
  pages = {828--834},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2134}
}
% == BibTeX quality report for Bamman2014a:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: ACL 2014
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/P14-2134
% ? Unused version: 167

@article{Baumgartner2020,
  title = {The {{Pushshift Reddit Dataset}}},
  author = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  year = {2020},
  month = jan,
  abstract = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.},
  archiveprefix = {arXiv},
  eprint = {2001.08435},
  eprinttype = {arxiv},
  journal = {arXiv:2001.08435 [cs]},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks},
  language = {en},
  primaryclass = {cs}
}
% == BibTeX quality report for Baumgartner2020:
% ? Possibly abbreviated journal title arXiv:2001.08435 [cs]
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: http://arxiv.org/abs/2001.08435
% ? Unused version: 164

@inproceedings{Burger2011,
  title = {Discriminating {{Gender}} on {{Twitter}}},
  booktitle = {Proceedings of the 2011 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Burger, John D. and Henderson, John and Kim, George and Zarrella, Guido},
  year = {2011},
  month = jul,
  pages = {1301--1309},
  publisher = {{Association for Computational Linguistics}},
  address = {{Edinburgh, Scotland, UK.}}
}
% == BibTeX quality report for Burger2011:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: EMNLP 2011
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/D11-1120
% ? Unused version: 172

@inproceedings{Ciot2013,
  title = {Gender {{Inference}} of {{Twitter Users}} in {{Non}}-{{English Contexts}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ciot, Morgane and Sonderegger, Morgan and Ruths, Derek},
  year = {2013},
  month = oct,
  pages = {1136--1145},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}}
}
% == BibTeX quality report for Ciot2013:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: EMNLP 2013
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/D13-1114
% ? Unused version: 172

@book{Clark1996,
  title = {Using {{Language}}},
  author = {Clark, Herbert H.},
  year = {1996},
  month = may,
  publisher = {{Cambridge University Press}},
  abstract = {Herbert Clark argues that language use is more than the sum of a speaker speaking and a listener listening. It is the joint action that emerges when speakers and listeners, writers and readers perform their individual actions in coordination, as ensembles. In contrast to work within the cognitive sciences, which has seen language use as an individual process, and to work within the social sciences, which has seen it as a social process, the author argues strongly that language use embodies both individual and social processes.},
  isbn = {978-0-521-56745-9},
  keywords = {common ground,Language Arts \& Disciplines / Linguistics / General,Language Arts \& Disciplines / Vocabulary},
  language = {en}
}
% == BibTeX quality report for Clark1996:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: Google Books
% ? Unused numPages: 452
% ? Unused version: 548

@inproceedings{DelTredici2017,
  title = {Semantic {{Variation}} in {{Online Communities}} of {{Practice}}},
  booktitle = {{{IWCS}} 2017 - 12th {{International Conference}} on {{Computational Semantics}} - {{Long}} Papers},
  author = {Del Tredici, Marco and Fern{\'a}ndez, Raquel},
  year = {2017}
}
% == BibTeX quality report for DelTredici2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/W17-6804
% ? Unused version: 163

@book{deSaussure2011,
  title = {Course in {{General Linguistics}}: {{Translated}} by {{Wade Baskin}}. {{Edited}} by {{Perry Meisel}} and {{Haun Saussy}}},
  shorttitle = {Course in {{General Linguistics}}},
  author = {{de Saussure}, Ferdinand},
  editor = {Meisel, Perry and Saussy, Haun},
  year = {2011},
  publisher = {{Columbia University Press}},
  abstract = {The founder of modern linguistics, Ferdinand de Saussure inaugurated semiology, structuralism, and deconstruction and made possible the work of Jacques Derrida, Roland Barthes, Michel Foucault, and Jacques Lacan, thus enabling the development of French feminism, gender studies, New Historicism, and postcolonialism. Based on Saussure's lectures, {$<$}em{$>$}Course in General Linguistics{$<$}/em{$>$} (1916) traces the rise and fall of the historical linguistics in which Saussure was trained, the synchronic or structural linguistics with which he replaced it, and the new look of diachronic linguistics that followed this change. Most important, Saussure presents the principles of a new linguistic science that includes the invention of semiology, or the theory of the "signifier," the "signified," and the "sign" that they combine to produce.  This is the first critical edition of  \emph{Course in General Linguistics}  to appear in English and restores Wade Baskin's original translation of 1959, in which the terms "signifier" and "signified" are introduced into English in this precise way. Baskin renders Saussure clearly and accessibly, allowing readers to experience his shift of the theory of reference from mimesis to performance and his expansion of poetics to include all media, including the life sciences and environmentalism. An introduction situates Saussure within the history of ideas and describes the history of scholarship that made {$<$}em{$>$}Course in General Linguistics{$<$}/em{$>$} legendary. New endnotes enlarge Saussure's contexts to include literary criticism, cultural studies, and philosophy.},
  translator = {Baskin, Wade}
}
% == BibTeX quality report for deSaussure2011:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: JSTOR
% ? Unused url: http://www.jstor.org/stable/10.7312/saus15726
% ? Unused version: 532

@inproceedings{Eisenstein2010,
  title = {A {{Latent Variable Model}} for {{Geographic Lexical Variation}}},
  booktitle = {Proceedings of the 2010 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A. and Xing, Eric P.},
  year = {2010},
  month = oct,
  pages = {1277--1287},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, MA}}
}
% == BibTeX quality report for Eisenstein2010:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: EMNLP 2010
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/D10-1124
% ? Unused version: 164

@article{Elazar2020,
  title = {Amnesic {{Probing}}: {{Behavioral Explanation}} with {{Amnesic Counterfactuals}}},
  shorttitle = {Amnesic {{Probing}}},
  author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
  year = {2020},
  month = dec,
  abstract = {A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method which focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention which removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.},
  archiveprefix = {arXiv},
  eprint = {2006.00995},
  eprinttype = {arxiv},
  journal = {arXiv:2006.00995 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryclass = {cs}
}
% == BibTeX quality report for Elazar2020:
% ? Possibly abbreviated journal title arXiv:2006.00995 [cs]
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: http://arxiv.org/abs/2006.00995
% ? Unused version: 203

@inproceedings{Feng2010,
  title = {Visual {{Information}} in {{Semantic Representation}}},
  author = {Feng, Yansong and Lapata, Mirella},
  year = {2010},
  month = jul,
  pages = {91--99},
  abstract = {The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account.}
}
% == BibTeX quality report for Feng2010:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: ResearchGate
% ? Unused version: 172

@book{Gower2004,
  title = {Procrustes Problems},
  author = {Gower, J. C. and Dijksterhuis, Garmt B.},
  year = {2004},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  annotation = {OCLC: ocm53156636},
  isbn = {978-0-19-851058-1},
  keywords = {Multivariate analysis},
  lccn = {QA278 .G686 2004},
  number = {30},
  series = {Oxford Statistical Science Series}
}

@incollection{Gumperz1972,
  title = {The {{Speech Community}}},
  booktitle = {Language and Social Context: Selected Readings},
  author = {Gumperz, J},
  editor = {Giglioli, Pier Paolo},
  year = {1972},
  publisher = {{Harmondsworth : Penguin}},
  abstract = {399 pages ; 18 cm; Includes bibliographical references; Hymes, D. Toward ethnographies of communication.--Fishman, J.A. The sociology of language.--Goffman, E. The neglected situation.--Basso, K.H. To give up on words: silence in Western Apache culture.--Frake, C.O. How to ask for a drink in Subanun.--Schegloff, E.A. Notes on a conversational practice: formulating place.--Searle, J. What is a speech act?--Bernstein, B. Social class, language and socialization.--Labov, W. The logic of nonstandard English.--Gumperz, J. The speech community.--Ferguson, C.A. Diglossia.--Brown, R. and Gilman, A. The pronouns of power and solidarity.--Labov, W. The study of language in its social context.--Goody, J. and Watt, I. The consequences of literacy.--Inglehart, R. and Woodward, M. Language conflicts and the political community},
  isbn = {978-0-14-080244-3 978-0-14-022649-2},
  keywords = {Sociolinguistics},
  language = {eng}
}
% == BibTeX quality report for Gumperz1972:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: Internet Archive
% ? Unused url: http://archive.org/details/languagesocialco0000unse
% ? Unused version: 171

@incollection{Hawkins2014,
  title = {Major Contributions from Formal Linguistics to the Complexity Debate},
  booktitle = {Measuring {{Grammatical Complexity}}},
  author = {Hawkins, John A.},
  editor = {Newmeyer, Frederick J. and Preston, Laurel B.},
  year = {2014},
  month = oct,
  pages = {14--36},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199685301.003.0002},
  isbn = {978-0-19-968530-1}
}

@article{Hochreiter1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journal = {Neural Computation},
  number = {8}
}
% == BibTeX quality report for Hochreiter1997:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: MIT Press Journals
% ? Unused url: https://doi.org/10.1162/neco.1997.9.8.1735
% ? Unused version: 171

@misc{Honnibal2017,
  title = {{{spaCy}} 2: {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  howpublished = {Explosion}
}

@inproceedings{Hovy2015,
  title = {Demographic {{Factors Improve Classification Performance}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hovy, Dirk},
  year = {2015},
  month = jul,
  pages = {752--762},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-1073}
}
% == BibTeX quality report for Hovy2015:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: ACL-IJCNLP 2015
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/P15-1073
% ? Unused version: 172

@inproceedings{Huang2014,
  title = {Enriching {{Cold Start Personalized Language Model Using Social Network Information}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Huang, Yu-Yang and Yan, Rui and Kuo, Tsung-Ting and Lin, Shou-De},
  year = {2014},
  pages = {611--617},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2100},
  abstract = {Personalized language models are useful in many applications, such as personalized search and personalized recommendation. Nevertheless, it is challenging to build a personalized language model for cold start users, in which the size of the training corpus of those users is too small to create a reasonably accurate and representative model. We introduce a generalized framework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users.},
  language = {en}
}
% == BibTeX quality report for Huang2014:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: DOI.org (Crossref)
% ? Unused url: http://aclweb.org/anthology/P14-2100
% ? Unused version: 462

@inproceedings{Kalchbrenner2013,
  title = {Recurrent {{Continuous Translation Models}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  year = {2013},
  pages = {10},
  address = {{Seattle, Washington}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {$>$} 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  language = {en}
}
% == BibTeX quality report for Kalchbrenner2013:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: EMNLP
% ? Unused libraryCatalog: Zotero
% ? Unused version: 164

@inproceedings{Kumar2018,
  title = {Community {{Interaction}} and {{Conflict}} on the {{Web}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}} - {{WWW}} '18},
  author = {Kumar, Srijan and Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2018},
  pages = {933--943},
  publisher = {{ACM Press}},
  address = {{Lyon, France}},
  doi = {10.1145/3178876.3186141},
  abstract = {Users organize themselves into communities on web platform60 s. These communities can interact with one another, often leading to conflicts and toxic interactions. However, little is known about the mechanisms of interactions between communities and how th40ey impact users.},
  isbn = {978-1-4503-5639-8},
  language = {en}
}
% == BibTeX quality report for Kumar2018:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: the 2018 World Wide Web Conference
% ? Unused libraryCatalog: DOI.org (Crossref)
% ? Unused url: http://dl.acm.org/citation.cfm?doid=3178876.3186141
% ? Unused version: 171

@article{Lau2017,
  title = {Grammaticality, {{Acceptability}}, and {{Probability}}: {{A Probabilistic View}} of {{Linguistic Knowledge}}},
  shorttitle = {Grammaticality, {{Acceptability}}, and {{Probability}}},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2017},
  volume = {41},
  pages = {1202--1241},
  issn = {1551-6709},
  doi = {10.1111/cogs.12414},
  abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12414},
  copyright = {Copyright \textcopyright{} 2016 Cognitive Science Society, Inc.},
  journal = {Cognitive Science},
  keywords = {Grammaticality,Probabilistic modeling,Syntactic knowledge},
  language = {en},
  number = {5}
}
% == BibTeX quality report for Lau2017:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: Wiley Online Library
% ? Unused url: http://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12414
% ? Unused version: 165

@inproceedings{Lau2017a,
  title = {Topically {{Driven Neural Language Model}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
  year = {2017},
  month = jul,
  pages = {355--365},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1033},
  abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.}
}
% == BibTeX quality report for Lau2017a:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: ACL 2017
% ? Unused libraryCatalog: ACLWeb
% ? Unused url: https://www.aclweb.org/anthology/P17-1033
% ? Unused version: 167

@article{Loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  journal = {arXiv:1711.05101 [cs, math]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  primaryclass = {cs, math}
}
% == BibTeX quality report for Loshchilov2019:
% ? Possibly abbreviated journal title arXiv:1711.05101 [cs, math]
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: http://arxiv.org/abs/1711.05101
% ? Unused version: 170

@article{Lucy2021,
  title = {Characterizing {{English Variation}} across {{Social Media Communities}} with {{BERT}}},
  author = {Lucy, Li and Bamman, David},
  year = {2021},
  month = may,
  volume = {9},
  pages = {538--556},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00383},
  abstract = {Much previous work characterizing language variation across Internet social groups has focused on the types of words used by these groups. We extend this type of study by employing BERT to characterize variation in the senses of words as well, analyzing two months of English comments in 474 Reddit communities. The specificity of different sense clusters to a community, combined with the specificity of a community's unique word types, is used to identify cases where a social group's language deviates from the norm. We validate our metrics using user-created glossaries and draw on sociolinguistic theories to connect language variation with trends in community behavior. We find that communities with highly distinctive language are medium-sized, and their loyal and highly engaged users interact in dense networks.},
  journal = {Transactions of the Association for Computational Linguistics}
}
% == BibTeX quality report for Lucy2021:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: Silverchair
% ? Unused url: https://doi.org/10.1162/tacl_a_00383
% ? Unused version: 526

@inproceedings{Martin2017,
  title = {Community2vec: {{Vector}} Representations of Online Communities Encode Semantic Relationships},
  shorttitle = {Community2vec},
  booktitle = {Proceedings of the {{Second Workshop}} on {{NLP}} and {{Computational Social Science}}},
  author = {Martin, Trevor},
  year = {2017},
  month = aug,
  pages = {27--31},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/W17-2904},
  abstract = {Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.}
}

@inproceedings{Nguyen2013,
  title = {How {{Old Do You Think I Am}}?: {{A Study}} of {{Language}} and {{Age}} in {{Twitter}}},
  booktitle = {Proceedings of the {{Seventh International AAAI Conference}} on {{Weblogs}} and {{Social Media}}},
  author = {Nguyen, Dong and Gravel, Rilana and Trieschnigg, Dolf and Meder, Theo},
  year = {2013},
  pages = {10},
  abstract = {In this paper we focus on the connection between age and language use, exploring age prediction of Twitter users based on their tweets. We discuss the construction of a fine-grained annotation effort to assign ages and life stages to Twitter users. Using this dataset, we explore age prediction in three different ways: classifying users into age categories, by life stages, and predicting their exact age. We find that an automatic system achieves better performance than humans on these tasks and that both humans and the automatic systems have difficulties predicting the age of older people. Moreover, we present a detailed analysis of variables that change with age. We find strong patterns of change, and that most changes occur at young ages.},
  language = {en}
}
% == BibTeX quality report for Nguyen2013:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: Zotero
% ? Unused version: 173

@inproceedings{OConnor2010,
  title = {A Mixture Model of Demographic Lexical Variation},
  booktitle = {In {{Proceedings}} of {{NIPS Workshop}} on {{Machine Learning}} for {{Social Computing}}},
  author = {O'Connor, Brendan and Eisenstein, Jacob and Xing, Eric P and Smith, Noah A},
  year = {2010},
  pages = {6},
  publisher = {{2010}},
  address = {{Vancouver, BC, Canada}},
  abstract = {We propose a Bayesian generative model of how demographic social factors influence lexical choice. We apply the method to a corpus of geo-tagged Twitter messages originating from mobile phones, cross-referenced against U.S. Census demographic data. Our method discovers communities jointly defined by linguistic and demographic properties.},
  language = {en}
}

@inproceedings{Peters2018a,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  pages = {1499--1509},
  address = {{Brussels, Belgium}},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  language = {en}
}
% == BibTeX quality report for Peters2018a:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused conferenceName: Conference on Empirical Methods in Natural Language Processing
% ? Unused libraryCatalog: Zotero
% ? Unused version: 285

@article{Stalnaker2002,
  title = {Common {{Ground}}},
  author = {Stalnaker, Robert},
  year = {2002},
  volume = {25},
  pages = {701--721},
  journal = {Linguistics and Philosophy},
  number = {5-6}
}
% == BibTeX quality report for Stalnaker2002:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused libraryCatalog: PhilPapers
% ? Unused version: 548

@article{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}
% == BibTeX quality report for Vaswani2017:
% ? Possibly abbreviated journal title arXiv:1706.03762 [cs]
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: http://arxiv.org/abs/1706.03762
% ? Unused version: 163

@article{Vinyals2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  month = apr,
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  archiveprefix = {arXiv},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  journal = {arXiv:1411.4555 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}
% == BibTeX quality report for Vinyals2015:
% ? Possibly abbreviated journal title arXiv:1411.4555 [cs]
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: http://arxiv.org/abs/1411.4555
% ? Unused version: 164

@article{Yang2017,
  title = {Overcoming {{Language Variation}} in {{Sentiment Analysis}} with {{Social Attention}}},
  author = {Yang, Yi and Eisenstein, Jacob},
  year = {2017},
  month = dec,
  volume = {5},
  pages = {295--307},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00062},
  abstract = {Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random; it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}
% == BibTeX quality report for Yang2017:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused journalAbbreviation: TACL
% ? Unused libraryCatalog: DOI.org (Crossref)
% ? Unused url: https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00062
% ? Unused version: 172


% Required packages:
% * textcomp

