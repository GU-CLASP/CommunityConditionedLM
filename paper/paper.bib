
@article{Bamman2014,
  title = {Gender Identity and Lexical Variation in Social Media},
  author = {Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler},
  year = {2014},
  volume = {18},
  pages = {135--160},
  issn = {1467-9841},
  doi = {10.1111/josl.12080},
  abstract = {We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/josl.12080},
  copyright = {\textcopyright{} 2014 John Wiley \& Sons Ltd},
  file = {/Users/xnobwi/.zotero/storage/CJSGAFPE/Bamman et al. - 2014 - Gender identity and lexical variation in social me.pdf},
  journal = {Journal of Sociolinguistics},
  keywords = {computational methods,computer-mediated communication,Gender,social media,social networks,style},
  language = {en},
  number = {2}
}

@inproceedings{Bamman2014a,
  title = {Distributed {{Representations}} of {{Geographically Situated Language}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Bamman, David and Dyer, Chris and Smith, Noah A.},
  year = {2014},
  month = jun,
  pages = {828--834},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2134},
  file = {/Users/xnobwi/.zotero/storage/5XXXZMPB/Bamman et al. - 2014 - Distributed Representations of Geographically Situ.pdf}
}

@article{Baumgartner2020,
  title = {The {{Pushshift Reddit Dataset}}},
  author = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  year = {2020},
  month = jan,
  abstract = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.},
  archivePrefix = {arXiv},
  eprint = {2001.08435},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/EKD4IX6E/Baumgartner et al. - 2020 - The Pushshift Reddit Dataset.pdf},
  journal = {arXiv:2001.08435 [cs]},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks},
  language = {en},
  primaryClass = {cs}
}

@book{Clark1996,
  title = {Using {{Language}}},
  author = {Clark, Herbert H.},
  year = {1996},
  month = may,
  publisher = {{Cambridge University Press}},
  abstract = {Herbert Clark argues that language use is more than the sum of a speaker speaking and a listener listening. It is the joint action that emerges when speakers and listeners, writers and readers perform their individual actions in coordination, as ensembles. In contrast to work within the cognitive sciences, which has seen language use as an individual process, and to work within the social sciences, which has seen it as a social process, the author argues strongly that language use embodies both individual and social processes.},
  file = {/Users/xnobwi/.zotero/storage/28T77585/05.3_pp_92_122_Common_ground.pdf;/Users/xnobwi/.zotero/storage/5L8QFUQC/06.1_pp_125_154_Meaning_and_understanding.pdf;/Users/xnobwi/.zotero/storage/5YQ72DEA/05.1_pp_29_58_Joint_activities.pdf;/Users/xnobwi/.zotero/storage/84GGEZBD/08.0_pp_287_288_Discourse.pdf;/Users/xnobwi/.zotero/storage/ATZEJCKU/04.0_pp_1_2_Introduction.pdf;/Users/xnobwi/.zotero/storage/B8E7JV8V/01.0_pp_i_vi_Frontmatter.pdf;/Users/xnobwi/.zotero/storage/BK4NNK52/08.2_pp_318_352_Conversation.pdf;/Users/xnobwi/.zotero/storage/DKKZEJMJ/09.1_pp_387_392_Conclusion.pdf;/Users/xnobwi/.zotero/storage/DWG39TIA/03.0_pp_ix_xii_Preface.pdf;/Users/xnobwi/.zotero/storage/ECLNYGCR/11.0_pp_413_418_Index_of_names.pdf;/Users/xnobwi/.zotero/storage/HTN5W94R/10.0_pp_393_412_References.pdf;/Users/xnobwi/.zotero/storage/I9ZV6J9U/06.0_pp_123_124_Communicative_acts.pdf;/Users/xnobwi/.zotero/storage/JEY8WR4L/07.2_pp_221_252_Grounding.pdf;/Users/xnobwi/.zotero/storage/JLAXX2KC/06.2_pp_155_188_Signaling.pdf;/Users/xnobwi/.zotero/storage/M7EY4D8R/07.0_pp_189_190_Levels_of_action.pdf;/Users/xnobwi/.zotero/storage/NBEZSAWH/08.3_pp_353_384_Layering.pdf;/Users/xnobwi/.zotero/storage/PHJJ4MC4/09.0_pp_385_386_Conclusion.pdf;/Users/xnobwi/.zotero/storage/RDGWGVIN/04.1_pp_3_26_Language_use.pdf;/Users/xnobwi/.zotero/storage/RFCMZKNY/07.1_pp_191_220_Joint_projects.pdf;/Users/xnobwi/.zotero/storage/TVR6KC5I/12.0_pp_419_432_Subject_index.pdf;/Users/xnobwi/.zotero/storage/U9BVVX58/05.0_pp_27_28_Foundations.pdf;/Users/xnobwi/.zotero/storage/UG6255PE/05.2_pp_59_91_Joint_actions.pdf;/Users/xnobwi/.zotero/storage/VLQL88T4/02.0_pp_vii_viii_Contents.pdf;/Users/xnobwi/.zotero/storage/WB8XLCZM/08.1_pp_289_317_Joint_commitment.pdf;/Users/xnobwi/.zotero/storage/XGUCEMUW/07.3_pp_253_286_Utterances.pdf},
  isbn = {978-0-521-56745-9},
  keywords = {common ground,Language Arts \& Disciplines / Linguistics / General,Language Arts \& Disciplines / Vocabulary},
  language = {en}
}

@inproceedings{DelTredici2017,
  title = {Semantic {{Variation}} in {{Online Communities}} of {{Practice}}},
  booktitle = {{{IWCS}} 2017 - 12th {{International Conference}} on {{Computational Semantics}} - {{Long}} Papers},
  author = {Del Tredici, Marco and Fern{\'a}ndez, Raquel},
  year = {2017},
  file = {/Users/xnobwi/.zotero/storage/ZM6ANXTV/Tredici and Fern√°ndez - 2017 - Semantic Variation in Online Communities of Practi.pdf}
}

@incollection{Gumperz1972,
  title = {The {{Speech Community}}},
  booktitle = {Language and Social Context: Selected Readings},
  author = {Gumperz, J},
  editor = {Giglioli, Pier Paolo},
  year = {1972},
  publisher = {{Harmondsworth : Penguin}},
  abstract = {399 pages ; 18 cm; Includes bibliographical references; Hymes, D. Toward ethnographies of communication.--Fishman, J.A. The sociology of language.--Goffman, E. The neglected situation.--Basso, K.H. To give up on words: silence in Western Apache culture.--Frake, C.O. How to ask for a drink in Subanun.--Schegloff, E.A. Notes on a conversational practice: formulating place.--Searle, J. What is a speech act?--Bernstein, B. Social class, language and socialization.--Labov, W. The logic of nonstandard English.--Gumperz, J. The speech community.--Ferguson, C.A. Diglossia.--Brown, R. and Gilman, A. The pronouns of power and solidarity.--Labov, W. The study of language in its social context.--Goody, J. and Watt, I. The consequences of literacy.--Inglehart, R. and Woodward, M. Language conflicts and the political community},
  isbn = {978-0-14-080244-3 978-0-14-022649-2},
  keywords = {Sociolinguistics},
  language = {eng}
}

@article{Hochreiter1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journal = {Neural Computation},
  number = {8}
}

@misc{Honnibal2017,
  title = {{{spaCy}} 2: {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  howpublished = {Explosion}
}

@inproceedings{Kumar2018,
  title = {Community {{Interaction}} and {{Conflict}} on the {{Web}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}} - {{WWW}} '18},
  author = {Kumar, Srijan and Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2018},
  pages = {933--943},
  publisher = {{ACM Press}},
  address = {{Lyon, France}},
  doi = {10.1145/3178876.3186141},
  abstract = {Users organize themselves into communities on web platform60 s. These communities can interact with one another, often leading to conflicts and toxic interactions. However, little is known about the mechanisms of interactions between communities and how th40ey impact users.},
  file = {/Users/xnobwi/.zotero/storage/IUCJX983/Kumar et al. - 2018 - Community Interaction and Conflict on the Web.pdf},
  isbn = {978-1-4503-5639-8},
  language = {en}
}

@article{Lau2017,
  title = {Grammaticality, {{Acceptability}}, and {{Probability}}: {{A Probabilistic View}} of {{Linguistic Knowledge}}},
  shorttitle = {Grammaticality, {{Acceptability}}, and {{Probability}}},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2017},
  volume = {41},
  pages = {1202--1241},
  issn = {1551-6709},
  doi = {10.1111/cogs.12414},
  abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12414},
  copyright = {Copyright \textcopyright{} 2016 Cognitive Science Society, Inc.},
  file = {/Users/xnobwi/.zotero/storage/8Q4WVS6A/Lau et al. - 2017 - Grammaticality, Acceptability, and Probability A .pdf},
  journal = {Cognitive Science},
  keywords = {Grammaticality,Probabilistic modeling,Syntactic knowledge},
  language = {en},
  number = {5}
}

@inproceedings{Lau2017a,
  title = {Topically {{Driven Neural Language Model}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
  year = {2017},
  month = jul,
  pages = {355--365},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1033},
  abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.},
  file = {/Users/xnobwi/.zotero/storage/UAR39YDR/Lau et al. - 2017 - Topically Driven Neural Language Model.pdf}
}

@article{Loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archivePrefix = {arXiv},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/M5NJK34K/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf},
  journal = {arXiv:1711.05101 [cs, math]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@inproceedings{Martin2017,
  title = {Community2vec: {{Vector}} Representations of Online Communities Encode Semantic Relationships},
  shorttitle = {Community2vec},
  booktitle = {Proceedings of the {{Second Workshop}} on {{NLP}} and {{Computational Social Science}}},
  author = {Martin, Trevor},
  year = {2017},
  month = aug,
  pages = {27--31},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/W17-2904},
  abstract = {Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.},
  file = {/Users/xnobwi/.zotero/storage/LITBFRMX/Martin - 2017 - community2vec Vector representations of online co.pdf}
}

@article{Stalnaker2002,
  title = {Common {{Ground}}},
  author = {Stalnaker, Robert},
  year = {2002},
  volume = {25},
  pages = {701--721},
  file = {/Users/xnobwi/.zotero/storage/NTSLH44N/Stalnaker-2002-Common_Ground.pdf},
  journal = {Linguistics and Philosophy},
  number = {5-6}
}

@article{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/IWEAZI3G/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}


