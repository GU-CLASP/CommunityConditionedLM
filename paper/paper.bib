
@article{Bamman2014,
  title = {Gender Identity and Lexical Variation in Social Media},
  author = {Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler},
  year = {2014},
  volume = {18},
  pages = {135--160},
  issn = {1467-9841},
  doi = {10.1111/josl.12080},
  abstract = {We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/josl.12080},
  copyright = {\textcopyright{} 2014 John Wiley \& Sons Ltd},
  file = {/Users/xnobwi/.zotero/storage/CJSGAFPE/Bamman et al. - 2014 - Gender identity and lexical variation in social me.pdf},
  journal = {Journal of Sociolinguistics},
  keywords = {computational methods,computer-mediated communication,Gender,social media,social networks,style},
  language = {en},
  number = {2}
}

@inproceedings{Bamman2014a,
  title = {Distributed {{Representations}} of {{Geographically Situated Language}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Bamman, David and Dyer, Chris and Smith, Noah A.},
  year = {2014},
  month = jun,
  pages = {828--834},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2134},
  file = {/Users/xnobwi/.zotero/storage/5XXXZMPB/Bamman et al. - 2014 - Distributed Representations of Geographically Situ.pdf}
}

@article{Baumgartner2020,
  title = {The {{Pushshift Reddit Dataset}}},
  author = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  year = {2020},
  month = jan,
  abstract = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.},
  archivePrefix = {arXiv},
  eprint = {2001.08435},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/EKD4IX6E/Baumgartner et al. - 2020 - The Pushshift Reddit Dataset.pdf},
  journal = {arXiv:2001.08435 [cs]},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Burger2011,
  title = {Discriminating {{Gender}} on {{Twitter}}},
  booktitle = {Proceedings of the 2011 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Burger, John D. and Henderson, John and Kim, George and Zarrella, Guido},
  year = {2011},
  month = jul,
  pages = {1301--1309},
  publisher = {{Association for Computational Linguistics}},
  address = {{Edinburgh, Scotland, UK.}},
  file = {/Users/xnobwi/.zotero/storage/B22CKXMW/Burger et al. - 2011 - Discriminating Gender on Twitter.pdf}
}

@inproceedings{Ciot2013,
  title = {Gender {{Inference}} of {{Twitter Users}} in {{Non}}-{{English Contexts}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ciot, Morgane and Sonderegger, Morgan and Ruths, Derek},
  year = {2013},
  month = oct,
  pages = {1136--1145},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}},
  file = {/Users/xnobwi/.zotero/storage/7JPMMHIW/Ciot et al. - 2013 - Gender Inference of Twitter Users in Non-English C.pdf}
}

@book{Clark1996,
  title = {Using {{Language}}},
  author = {Clark, Herbert H.},
  year = {1996},
  month = may,
  publisher = {{Cambridge University Press}},
  abstract = {Herbert Clark argues that language use is more than the sum of a speaker speaking and a listener listening. It is the joint action that emerges when speakers and listeners, writers and readers perform their individual actions in coordination, as ensembles. In contrast to work within the cognitive sciences, which has seen language use as an individual process, and to work within the social sciences, which has seen it as a social process, the author argues strongly that language use embodies both individual and social processes.},
  file = {/Users/xnobwi/.zotero/storage/28T77585/05.3_pp_92_122_Common_ground.pdf;/Users/xnobwi/.zotero/storage/5L8QFUQC/06.1_pp_125_154_Meaning_and_understanding.pdf;/Users/xnobwi/.zotero/storage/5YQ72DEA/05.1_pp_29_58_Joint_activities.pdf;/Users/xnobwi/.zotero/storage/84GGEZBD/08.0_pp_287_288_Discourse.pdf;/Users/xnobwi/.zotero/storage/ATZEJCKU/04.0_pp_1_2_Introduction.pdf;/Users/xnobwi/.zotero/storage/B8E7JV8V/01.0_pp_i_vi_Frontmatter.pdf;/Users/xnobwi/.zotero/storage/BK4NNK52/08.2_pp_318_352_Conversation.pdf;/Users/xnobwi/.zotero/storage/DKKZEJMJ/09.1_pp_387_392_Conclusion.pdf;/Users/xnobwi/.zotero/storage/DWG39TIA/03.0_pp_ix_xii_Preface.pdf;/Users/xnobwi/.zotero/storage/ECLNYGCR/11.0_pp_413_418_Index_of_names.pdf;/Users/xnobwi/.zotero/storage/HTN5W94R/10.0_pp_393_412_References.pdf;/Users/xnobwi/.zotero/storage/I9ZV6J9U/06.0_pp_123_124_Communicative_acts.pdf;/Users/xnobwi/.zotero/storage/JEY8WR4L/07.2_pp_221_252_Grounding.pdf;/Users/xnobwi/.zotero/storage/JLAXX2KC/06.2_pp_155_188_Signaling.pdf;/Users/xnobwi/.zotero/storage/M7EY4D8R/07.0_pp_189_190_Levels_of_action.pdf;/Users/xnobwi/.zotero/storage/NBEZSAWH/08.3_pp_353_384_Layering.pdf;/Users/xnobwi/.zotero/storage/PHJJ4MC4/09.0_pp_385_386_Conclusion.pdf;/Users/xnobwi/.zotero/storage/RDGWGVIN/04.1_pp_3_26_Language_use.pdf;/Users/xnobwi/.zotero/storage/RFCMZKNY/07.1_pp_191_220_Joint_projects.pdf;/Users/xnobwi/.zotero/storage/TVR6KC5I/12.0_pp_419_432_Subject_index.pdf;/Users/xnobwi/.zotero/storage/U9BVVX58/05.0_pp_27_28_Foundations.pdf;/Users/xnobwi/.zotero/storage/UG6255PE/05.2_pp_59_91_Joint_actions.pdf;/Users/xnobwi/.zotero/storage/VLQL88T4/02.0_pp_vii_viii_Contents.pdf;/Users/xnobwi/.zotero/storage/WB8XLCZM/08.1_pp_289_317_Joint_commitment.pdf;/Users/xnobwi/.zotero/storage/XGUCEMUW/07.3_pp_253_286_Utterances.pdf},
  isbn = {978-0-521-56745-9},
  keywords = {common ground,Language Arts \& Disciplines / Linguistics / General,Language Arts \& Disciplines / Vocabulary},
  language = {en}
}

@inproceedings{DelTredici2017,
  title = {Semantic {{Variation}} in {{Online Communities}} of {{Practice}}},
  booktitle = {{{IWCS}} 2017 - 12th {{International Conference}} on {{Computational Semantics}} - {{Long}} Papers},
  author = {Del Tredici, Marco and Fern{\'a}ndez, Raquel},
  year = {2017},
  file = {/Users/xnobwi/.zotero/storage/ZM6ANXTV/Tredici and Fernández - 2017 - Semantic Variation in Online Communities of Practi.pdf}
}

@inproceedings{Eisenstein2010,
  title = {A {{Latent Variable Model}} for {{Geographic Lexical Variation}}},
  booktitle = {Proceedings of the 2010 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A. and Xing, Eric P.},
  year = {2010},
  month = oct,
  pages = {1277--1287},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, MA}},
  file = {/Users/xnobwi/.zotero/storage/87VK9SZ9/Eisenstein et al. - 2010 - A Latent Variable Model for Geographic Lexical Var.pdf}
}

@article{Elazar2020,
  title = {Amnesic {{Probing}}: {{Behavioral Explanation}} with {{Amnesic Counterfactuals}}},
  shorttitle = {Amnesic {{Probing}}},
  author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
  year = {2020},
  month = dec,
  abstract = {A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method which focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention which removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.},
  archivePrefix = {arXiv},
  eprint = {2006.00995},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/5ZVIPC7M/Elazar et al. - 2020 - Amnesic Probing Behavioral Explanation with Amnes.pdf},
  journal = {arXiv:2006.00995 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Feng2010,
  title = {Visual {{Information}} in {{Semantic Representation}}},
  author = {Feng, Yansong and Lapata, Mirella},
  year = {2010},
  month = jul,
  pages = {91--99},
  abstract = {The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account.},
  file = {/Users/xnobwi/.zotero/storage/WJP8FY9X/Feng and Lapata - Visual Information in Semantic Representation.pdf}
}

@incollection{Gumperz1972,
  title = {The {{Speech Community}}},
  booktitle = {Language and Social Context: Selected Readings},
  author = {Gumperz, J},
  editor = {Giglioli, Pier Paolo},
  year = {1972},
  publisher = {{Harmondsworth : Penguin}},
  abstract = {399 pages ; 18 cm; Includes bibliographical references; Hymes, D. Toward ethnographies of communication.--Fishman, J.A. The sociology of language.--Goffman, E. The neglected situation.--Basso, K.H. To give up on words: silence in Western Apache culture.--Frake, C.O. How to ask for a drink in Subanun.--Schegloff, E.A. Notes on a conversational practice: formulating place.--Searle, J. What is a speech act?--Bernstein, B. Social class, language and socialization.--Labov, W. The logic of nonstandard English.--Gumperz, J. The speech community.--Ferguson, C.A. Diglossia.--Brown, R. and Gilman, A. The pronouns of power and solidarity.--Labov, W. The study of language in its social context.--Goody, J. and Watt, I. The consequences of literacy.--Inglehart, R. and Woodward, M. Language conflicts and the political community},
  isbn = {978-0-14-080244-3 978-0-14-022649-2},
  keywords = {Sociolinguistics},
  language = {eng}
}

@article{Hochreiter1997,
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journal = {Neural Computation},
  number = {8}
}

@misc{Honnibal2017,
  title = {{{spaCy}} 2: {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  howpublished = {Explosion}
}

@inproceedings{Hovy2015,
  title = {Demographic {{Factors Improve Classification Performance}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hovy, Dirk},
  year = {2015},
  month = jul,
  pages = {752--762},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-1073},
  file = {/Users/xnobwi/.zotero/storage/ZF3IBTY5/Hovy - 2015 - Demographic Factors Improve Classification Perform.pdf}
}

@inproceedings{Kalchbrenner2013,
  title = {Recurrent {{Continuous Translation Models}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  year = {2013},
  pages = {10},
  address = {{Seattle, Washington}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {$>$} 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  file = {/Users/xnobwi/.zotero/storage/54N2LJYG/Kalchbrenner and Blunsom - Recurrent Continuous Translation Models.pdf},
  language = {en}
}

@inproceedings{Kumar2018,
  title = {Community {{Interaction}} and {{Conflict}} on the {{Web}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}} - {{WWW}} '18},
  author = {Kumar, Srijan and Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2018},
  pages = {933--943},
  publisher = {{ACM Press}},
  address = {{Lyon, France}},
  doi = {10.1145/3178876.3186141},
  abstract = {Users organize themselves into communities on web platform60 s. These communities can interact with one another, often leading to conflicts and toxic interactions. However, little is known about the mechanisms of interactions between communities and how th40ey impact users.},
  file = {/Users/xnobwi/.zotero/storage/IUCJX983/Kumar et al. - 2018 - Community Interaction and Conflict on the Web.pdf},
  isbn = {978-1-4503-5639-8},
  language = {en}
}

@article{Lau2017,
  title = {Grammaticality, {{Acceptability}}, and {{Probability}}: {{A Probabilistic View}} of {{Linguistic Knowledge}}},
  shorttitle = {Grammaticality, {{Acceptability}}, and {{Probability}}},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2017},
  volume = {41},
  pages = {1202--1241},
  issn = {1551-6709},
  doi = {10.1111/cogs.12414},
  abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12414},
  copyright = {Copyright \textcopyright{} 2016 Cognitive Science Society, Inc.},
  file = {/Users/xnobwi/.zotero/storage/8Q4WVS6A/Lau et al. - 2017 - Grammaticality, Acceptability, and Probability A .pdf},
  journal = {Cognitive Science},
  keywords = {Grammaticality,Probabilistic modeling,Syntactic knowledge},
  language = {en},
  number = {5}
}

@inproceedings{Lau2017a,
  title = {Topically {{Driven Neural Language Model}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
  year = {2017},
  month = jul,
  pages = {355--365},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1033},
  abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.},
  file = {/Users/xnobwi/.zotero/storage/UAR39YDR/Lau et al. - 2017 - Topically Driven Neural Language Model.pdf}
}

@article{Loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archivePrefix = {arXiv},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/M5NJK34K/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf},
  journal = {arXiv:1711.05101 [cs, math]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@inproceedings{Martin2017,
  title = {Community2vec: {{Vector}} Representations of Online Communities Encode Semantic Relationships},
  shorttitle = {Community2vec},
  booktitle = {Proceedings of the {{Second Workshop}} on {{NLP}} and {{Computational Social Science}}},
  author = {Martin, Trevor},
  year = {2017},
  month = aug,
  pages = {27--31},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/W17-2904},
  abstract = {Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.},
  file = {/Users/xnobwi/.zotero/storage/LITBFRMX/Martin - 2017 - community2vec Vector representations of online co.pdf}
}

@inproceedings{Nguyen2013,
  title = {How {{Old Do You Think I Am}}?: {{A Study}} of {{Language}} and {{Age}} in {{Twitter}}},
  booktitle = {Proceedings of the {{Seventh International AAAI Conference}} on {{Weblogs}} and {{Social Media}}},
  author = {Nguyen, Dong and Gravel, Rilana and Trieschnigg, Dolf and Meder, Theo},
  year = {2013},
  pages = {10},
  abstract = {In this paper we focus on the connection between age and language use, exploring age prediction of Twitter users based on their tweets. We discuss the construction of a fine-grained annotation effort to assign ages and life stages to Twitter users. Using this dataset, we explore age prediction in three different ways: classifying users into age categories, by life stages, and predicting their exact age. We find that an automatic system achieves better performance than humans on these tasks and that both humans and the automatic systems have difficulties predicting the age of older people. Moreover, we present a detailed analysis of variables that change with age. We find strong patterns of change, and that most changes occur at young ages.},
  file = {/Users/xnobwi/.zotero/storage/Z2Q3SU8Z/Nguyen et al. - How Old Do You Think I Am A Study of Language an.pdf},
  language = {en}
}

@inproceedings{OConnor2010,
  title = {A Mixture Model of Demographic Lexical Variation},
  booktitle = {In {{Proceedings}} of {{NIPS Workshop}} on {{Machine Learning}} for {{Social Computing}}},
  author = {O'Connor, Brendan and Eisenstein, Jacob and Xing, Eric P and Smith, Noah A},
  year = {2010},
  pages = {6},
  publisher = {{2010}},
  address = {{Vancouver, BC, Canada}},
  abstract = {We propose a Bayesian generative model of how demographic social factors influence lexical choice. We apply the method to a corpus of geo-tagged Twitter messages originating from mobile phones, cross-referenced against U.S. Census demographic data. Our method discovers communities jointly defined by linguistic and demographic properties.},
  file = {/Users/xnobwi/.zotero/storage/YPEY7ZJ6/O’Connor et al. - Discovering Demographic Language Variation.pdf},
  language = {en}
}

@inproceedings{Peters2018a,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  pages = {1499--1509},
  address = {{Brussels, Belgium}},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  file = {/Users/xnobwi/.zotero/storage/Z6JP4TYC/Peters et al. - Dissecting Contextual Word Embeddings Architectur.pdf},
  language = {en}
}

@article{Stalnaker2002,
  title = {Common {{Ground}}},
  author = {Stalnaker, Robert},
  year = {2002},
  volume = {25},
  pages = {701--721},
  file = {/Users/xnobwi/.zotero/storage/NTSLH44N/Stalnaker-2002-Common_Ground.pdf},
  journal = {Linguistics and Philosophy},
  number = {5-6}
}

@article{Vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/IWEAZI3G/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Vinyals2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  month = apr,
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/SDGLT9PD/Vinyals et al. - 2015 - Show and Tell A Neural Image Caption Generator.pdf},
  journal = {arXiv:1411.4555 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{Yang2017,
  title = {Overcoming {{Language Variation}} in {{Sentiment Analysis}} with {{Social Attention}}},
  author = {Yang, Yi and Eisenstein, Jacob},
  year = {2017},
  month = dec,
  volume = {5},
  pages = {295--307},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00062},
  abstract = {Variation in language is ubiquitous, particularly in newer forms of writing such as social media. Fortunately, variation is not random; it is often linked to social properties of the author. In this paper, we show how to exploit social networks to make sentiment analysis more robust to social language variation. The key idea is linguistic homophily: the tendency of socially linked individuals to use language in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author's position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data.},
  file = {/Users/xnobwi/.zotero/storage/FICQCJTQ/Yang and Eisenstein - 2017 - Overcoming Language Variation in Sentiment Analysi.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}


