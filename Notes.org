

* On what level of linguistic analysis is variation is happening between communities?

Experiments:
 - train transformer model with added community embedding at various levels.
 - test if the model can predict which community a message is coming from
   - leading idea:
     - to get the community embedding, multiply matrices of embedding by a weight W.
       - in training W is a one-hot encoding of the community. 
         - (Data is the whole training set)
       - in testing, let W be optimised (and the rest is fixed)
         - Then entropy of W (seen as a distribution) gives a level of confidence about the community.
         - data is: a single input. (and all the things that were
           parameter before are fixed and thus treated as input data)

* Analysis of model quality

- Difference in information gains for simple/complex models
- Plot loss for each community (histogram or scatterplot for 2 variables)
  - eg. bigram model loss vs. transformer model loss.



* Analysis of community embeddings



- Correlation with the extra-linguistic properties of communities?.
  - for example using a single dense layer + softmax
  - Frequency of image posts?
  - Mean number of post per user. (or the correponding power law
    exponent estimator:
    https://en.wikipedia.org/wiki/Power_law#Maximum_likelihood)
  - Proportion of comments which are replies to other comments
  - Community/People co-occurence matrix / LSA


- Correlation between embeddings of several models.
  - Basic idea: check the Pearson collelation beween <c^m_i, c^m_j> for every
    pair of community (j,i), between to models (m).
  - If r=1, then you have a perfect orthogonal mapping
  - This is a very strong condition, because all dimensions play some role.
    -> Other idea is to do a projection on the n most relevant dimensions first.
       - For n=1 this is checking that the most relevant dimension is the same for both models

* Open questions:
-  Where in the architecture should the embedding go in general
- Can the topically-driven language model smooth the embedding
